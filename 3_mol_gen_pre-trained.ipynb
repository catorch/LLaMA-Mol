{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from typing_extensions import Self\n",
    "from lightning.fabric.strategies import DeepSpeedStrategy, FSDPStrategy\n",
    "from torch import distributed as dist\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaskCache = torch.Tensor\n",
    "RoPECache = torch.Tensor\n",
    "KVCache = Tuple[torch.Tensor, torch.Tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_multiple(n: int, k: int) -> int:\n",
    "    if n % k == 0:\n",
    "        return n\n",
    "    return n + k - (n % k)\n",
    "\n",
    "def save_model_checkpoint(model, file_path):\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    # Check if distributed training is initialized, if not, save the model directly\n",
    "    if not dist.is_initialized() or dist.get_rank() == 0:\n",
    "        state_dict = model.state_dict()\n",
    "        torch.save(state_dict, file_path)\n",
    "\n",
    "    # If distributed training is initialized, do a synchronization barrier after saving the model\n",
    "    if dist.is_initialized():\n",
    "        dist.barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_configs = {\n",
    "    \"7B\": dict(n_layer=32, n_head=32, n_embd=1024),\n",
    "    \"13B\": dict(n_layer=40, n_head=40, n_embd=5120),\n",
    "    \"30B\": dict(n_layer=60, n_head=52, n_embd=6656),\n",
    "    \"65B\": dict(n_layer=80, n_head=64, n_embd=8192),\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LLaMAConfig:\n",
    "    block_size: int = 2048\n",
    "    vocab_size: int = 32000\n",
    "    padded_vocab_size: Optional[int] = None\n",
    "    n_layer: int = 32\n",
    "    n_head: int = 32\n",
    "    n_embd: int = 4096\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.padded_vocab_size is None:\n",
    "            self.padded_vocab_size = find_multiple(self.vocab_size, 64)\n",
    "\n",
    "    @classmethod\n",
    "    def from_name(cls, name: str) -> Self:\n",
    "        return cls(**llama_configs[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLaMA(nn.Module):\n",
    "    def __init__(self, config: LLaMAConfig) -> None:\n",
    "        super().__init__()\n",
    "        assert config.padded_vocab_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.padded_vocab_size, bias=False)\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte=nn.Embedding(config.padded_vocab_size, config.n_embd),\n",
    "                h=nn.ModuleList(Block(config) for _ in range(config.n_layer)),\n",
    "                ln_f=RMSNorm(config.n_embd),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.rope_cache: Optional[RoPECache] = None\n",
    "        self.mask_cache: Optional[MaskCache] = None\n",
    "        self.kv_caches: List[KVCache] = []\n",
    "\n",
    "    def _init_weights(self, module: nn.Module) -> None:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layer))\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layer))\n",
    "\n",
    "    def forward(\n",
    "        self, idx: torch.Tensor, max_seq_length: Optional[int] = None, input_pos: Optional[torch.Tensor] = None\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, List[KVCache]]]:\n",
    "        B, T = idx.size()\n",
    "\n",
    "        block_size = self.config.block_size\n",
    "        if max_seq_length is None:\n",
    "            max_seq_length = block_size\n",
    "        assert T <= max_seq_length, f\"Cannot forward sequence of length {T}, max seq length is only {max_seq_length}\"\n",
    "        assert max_seq_length <= block_size, f\"Cannot attend to {max_seq_length}, block size is only {block_size}\"\n",
    "        assert T <= block_size, f\"Cannot forward sequence of length {T}, block size is only {block_size}\"\n",
    "\n",
    "        if self.rope_cache is None:\n",
    "            self.rope_cache = self.build_rope_cache(idx)\n",
    "        if self.mask_cache is None:\n",
    "            self.mask_cache = self.build_mask_cache(idx)\n",
    "\n",
    "        if input_pos is not None:\n",
    "            rope = self.rope_cache.index_select(0, input_pos)\n",
    "            mask = self.mask_cache.index_select(2, input_pos)\n",
    "            mask = mask[:, :, :, :max_seq_length]\n",
    "        else:\n",
    "            rope = self.rope_cache[:T]\n",
    "            mask = self.mask_cache[:, :, :T, :T]\n",
    "\n",
    "        # forward the model itself\n",
    "        x = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n",
    "\n",
    "        if input_pos is None:  # proxy for use_cache=False\n",
    "            for block in self.transformer.h:\n",
    "                x, _ = block(x, rope, mask, max_seq_length)\n",
    "        else:\n",
    "            if not self.kv_caches:\n",
    "                head_size = self.config.n_embd // self.config.n_head\n",
    "                cache_shape = (B, self.config.n_head, max_seq_length, head_size)\n",
    "                self.kv_caches = [\n",
    "                    (torch.zeros(cache_shape, device=x.device, dtype=x.dtype), torch.zeros(cache_shape, device=x.device, dtype=x.dtype))\n",
    "                    for _ in range(self.config.n_layer)\n",
    "                ]\n",
    "            for i, block in enumerate(self.transformer.h):\n",
    "                x, self.kv_caches[i] = block(x, rope, mask, max_seq_length, input_pos, self.kv_caches[i])\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        logits = self.lm_head(x)  # (b, t, vocab_size)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    @classmethod\n",
    "    def from_name(cls, name: str) -> Self:\n",
    "        return cls(LLaMAConfig.from_name(name))\n",
    "\n",
    "    def build_rope_cache(self, idx: torch.Tensor) -> RoPECache:\n",
    "        return build_rope_cache(\n",
    "            seq_len=self.config.block_size,\n",
    "            n_elem=self.config.n_embd // self.config.n_head,\n",
    "            dtype=idx.dtype,\n",
    "            device=idx.device,\n",
    "        )\n",
    "\n",
    "    def build_mask_cache(self, idx: torch.Tensor) -> MaskCache:\n",
    "        ones = torch.ones((self.config.block_size, self.config.block_size), device=idx.device, dtype=torch.bool)\n",
    "        return torch.tril(ones).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    def reset_cache(self) -> None:\n",
    "        self.kv_caches.clear()\n",
    "        if self.mask_cache.device.type == \"xla\":\n",
    "            # https://github.com/Lightning-AI/lit-parrot/pull/83#issuecomment-1558150179\n",
    "            self.rope_cache = None\n",
    "            self.mask_cache = None\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config: LLaMAConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.rms_1 = RMSNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.rms_2 = RMSNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        rope: RoPECache,\n",
    "        mask: MaskCache,\n",
    "        max_seq_length: int,\n",
    "        input_pos: Optional[torch.Tensor] = None,\n",
    "        kv_cache: Optional[KVCache] = None,\n",
    "    ) -> Tuple[torch.Tensor, Optional[KVCache]]:\n",
    "        h, new_kv_cache = self.attn(self.rms_1(x), rope, mask, max_seq_length, input_pos, kv_cache)\n",
    "        x = x + h\n",
    "        x = x + self.mlp(self.rms_2(x))\n",
    "        return x, new_kv_cache\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config: LLaMAConfig) -> None:\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.block_size = config.block_size\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        rope: RoPECache,\n",
    "        mask: MaskCache,\n",
    "        max_seq_length: int,\n",
    "        input_pos: Optional[torch.Tensor] = None,\n",
    "        kv_cache: Optional[KVCache] = None,\n",
    "    ) -> Tuple[torch.Tensor, Optional[KVCache]]:\n",
    "        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "\n",
    "        head_size = C // self.n_head\n",
    "        k = k.view(B, T, self.n_head, head_size)\n",
    "        q = q.view(B, T, self.n_head, head_size)\n",
    "        v = v.view(B, T, self.n_head, head_size)\n",
    "\n",
    "        q = apply_rope(q, rope)\n",
    "        k = apply_rope(k, rope)\n",
    "\n",
    "        k = k.transpose(1, 2)  # (B, nh, T, hs)\n",
    "        q = q.transpose(1, 2)  # (B, nh, T, hs)\n",
    "        v = v.transpose(1, 2)  # (B, nh, T, hs)\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            cache_k, cache_v = kv_cache\n",
    "            # check if reached token limit\n",
    "            if input_pos[-1] >= max_seq_length:\n",
    "                input_pos = torch.tensor(max_seq_length - 1, device=input_pos.device)\n",
    "                # shift 1 position to the left\n",
    "                cache_k = torch.roll(cache_k, -1, dims=2)\n",
    "                cache_v = torch.roll(cache_v, -1, dims=2)\n",
    "            k = cache_k.index_copy(2, input_pos, k)\n",
    "            v = cache_v.index_copy(2, input_pos, v)\n",
    "            kv_cache = k, v\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        #  att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        #  att = att.masked_fill(mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "        #  att = F.softmax(att, dim=-1)\n",
    "        #  y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        # efficient attention using Flash Attention CUDA kernels\n",
    "        y = F.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=0.0)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "\n",
    "        return y, kv_cache\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: LLaMAConfig) -> None:\n",
    "        super().__init__()\n",
    "        hidden_dim = 4 * config.n_embd\n",
    "        n_hidden = int(2 * hidden_dim / 3)\n",
    "        n_hidden = find_multiple(n_hidden, 256)\n",
    "\n",
    "        self.c_fc1 = nn.Linear(config.n_embd, n_hidden, bias=False)\n",
    "        self.c_fc2 = nn.Linear(config.n_embd, n_hidden, bias=False)\n",
    "        self.c_proj = nn.Linear(n_hidden, config.n_embd, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.silu(self.c_fc1(x)) * self.c_fc2(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization.\n",
    "\n",
    "    Derived from https://github.com/bzhangGo/rmsnorm/blob/master/rmsnorm_torch.py. BSD 3-Clause License:\n",
    "    https://github.com/bzhangGo/rmsnorm/blob/master/LICENSE.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size: int, dim: int = -1, eps: float = 1e-5) -> None:\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(size))\n",
    "        self.eps = eps\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # NOTE: the original RMSNorm paper implementation is not equivalent\n",
    "        # norm_x = x.norm(2, dim=self.dim, keepdim=True)\n",
    "        # rms_x = norm_x * d_x ** (-1. / 2)\n",
    "        # x_normed = x / (rms_x + self.eps)\n",
    "        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)\n",
    "        x_normed = x * torch.rsqrt(norm_x + self.eps)\n",
    "        return self.scale * x_normed\n",
    "\n",
    "\n",
    "def build_rope_cache(\n",
    "    seq_len: int, n_elem: int, dtype: torch.dtype, device: torch.device, base: int = 10000\n",
    ") -> RoPECache:\n",
    "    \"\"\"Enhanced Transformer with Rotary Position Embedding.\n",
    "\n",
    "    Derived from: https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/\n",
    "    transformers/rope/__init__.py. MIT License:\n",
    "    https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/license.\n",
    "    \"\"\"\n",
    "    # $\\Theta = {\\theta_i = 10000^{\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n",
    "    theta = 1.0 / (base ** (torch.arange(0, n_elem, 2, dtype=dtype, device=device) / n_elem))\n",
    "\n",
    "    # Create position indexes `[0, 1, ..., seq_len - 1]`\n",
    "    seq_idx = torch.arange(seq_len, dtype=dtype, device=device)\n",
    "\n",
    "    # Calculate the product of position index and $\\theta_i$\n",
    "    idx_theta = torch.outer(seq_idx, theta).float()\n",
    "\n",
    "    cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)\n",
    "\n",
    "    # this is to mimic the behaviour of complex32, else we will get different results\n",
    "    if dtype in (torch.float16, torch.bfloat16, torch.int8):\n",
    "        cache = cache.half()\n",
    "    return cache\n",
    "\n",
    "\n",
    "def apply_rope(x: torch.Tensor, rope_cache: RoPECache) -> torch.Tensor:\n",
    "    # truncate to support variable sizes\n",
    "    T = x.size(1)\n",
    "    rope_cache = rope_cache[:T]\n",
    "\n",
    "    # cast because the reference does\n",
    "    xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n",
    "    rope_cache = rope_cache.view(1, xshaped.size(1), 1, xshaped.size(3), 2)\n",
    "    x_out2 = torch.stack(\n",
    "        [\n",
    "            xshaped[..., 0] * rope_cache[..., 0] - xshaped[..., 1] * rope_cache[..., 1],\n",
    "            xshaped[..., 1] * rope_cache[..., 0] + xshaped[..., 0] * rope_cache[..., 1],\n",
    "        ],\n",
    "        -1,\n",
    "    )\n",
    "\n",
    "    x_out2 = x_out2.flatten(3)\n",
    "    return x_out2.type_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from sentencepiece import SentencePieceProcessor, SentencePieceTrainer\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"Tokenizer for LLaMA.\"\"\"\n",
    "\n",
    "    def __init__(self, model_path: Path) -> None:\n",
    "        self.processor = SentencePieceProcessor(model_file=str(model_path))\n",
    "        self.bos_id = self.processor.bos_id()\n",
    "        self.eos_id = self.processor.eos_id()\n",
    "        self.pad_id = self.processor.pad_id()\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return self.processor.vocab_size()\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        string: str,\n",
    "        bos: bool = True,\n",
    "        eos: bool = True,\n",
    "        max_length: int = -1,\n",
    "        pad: bool = False,\n",
    "        device: Optional[torch.device] = None\n",
    "    ) -> torch.Tensor:\n",
    "        lines = string.splitlines()\n",
    "        all_tokens = []\n",
    "        for line in lines:\n",
    "            tokens = self.processor.encode(line)\n",
    "            if bos:\n",
    "                tokens = [self.bos_id] + tokens\n",
    "            if eos:\n",
    "                tokens = tokens + [self.eos_id]\n",
    "            if max_length > 0:\n",
    "                tokens = tokens[:max_length]\n",
    "            if pad and len(tokens) < max_length:\n",
    "                tokens += [self.pad_id] * (max_length - len(tokens))\n",
    "            all_tokens.extend(tokens)\n",
    "        return torch.tensor(all_tokens, dtype=torch.int, device=device)\n",
    "\n",
    "    def decode(self, tokens: torch.Tensor) -> str:\n",
    "        return self.processor.decode(tokens.tolist())\n",
    "    \n",
    "    @staticmethod\n",
    "    def train(input: str, destination: str, vocab_size=32000) -> None:\n",
    "        model_prefix = os.path.join(destination, \"tokenizer\")\n",
    "        SentencePieceTrainer.Train(input=input, model_prefix=model_prefix, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "block_size = 1024\n",
    "config = LLaMAConfig.from_name(\"7B\")\n",
    "config.block_size = block_size\n",
    "config.vocab_size = 100  # from prepare_shakespeare.py\n",
    "\n",
    "# Model initialization\n",
    "model = LLaMA(config)\n",
    "# model.load_state_dict(torch.load(\"./out/training/iter-460000-ckpt.pth\"))\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model ...\n",
      "Time to load model: 7.06 seconds.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import torch\n",
    "import lightning as L\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "import functools\n",
    "import pickle\n",
    "\n",
    "def llama_model_lookup(checkpoint: dict) -> str:\n",
    "    \"\"\"Returns the LLaMA model name from the checkpoint.\n",
    "    \n",
    "    Checks the width of the lm_head.weight matrix, as these uniquely identify the model.\n",
    "    \"\"\"\n",
    "    embedding_size = checkpoint['transformer.wte.weight'].shape[1]\n",
    "    return llama_model_sizes[embedding_size]\n",
    "class NotYetLoadedTensor:\n",
    "    def __init__(self, metatensor, archiveinfo, storageinfo, rebuild_args):\n",
    "        self.metatensor = metatensor\n",
    "        self.archiveinfo = archiveinfo\n",
    "        self.storageinfo = storageinfo\n",
    "        self.rebuild_args = rebuild_args\n",
    "\n",
    "    @classmethod\n",
    "    def rebuild_from_type_v2(cls, func, new_type, args, state, *, archiveinfo=None):\n",
    "        ret = func(*args)\n",
    "        if isinstance(ret, NotYetLoadedTensor):\n",
    "            old_lt = ret._load_tensor\n",
    "\n",
    "            def _load_tensor():\n",
    "                t = old_lt()\n",
    "                return torch._tensor._rebuild_from_type_v2(\n",
    "                    lambda: t, new_type, (), state\n",
    "                )\n",
    "\n",
    "            ret._load_tensor = _load_tensor\n",
    "            return ret\n",
    "        return torch._tensor._rebuild_from_type_v2(func, new_type, args, state)\n",
    "\n",
    "    @classmethod\n",
    "    def rebuild_parameter(\n",
    "        cls, data, requires_grad, backward_hooks, *, archiveinfo=None\n",
    "    ):\n",
    "        if isinstance(data, NotYetLoadedTensor):\n",
    "            old_lt = data._load_tensor\n",
    "\n",
    "            def _load_tensor():\n",
    "                t = old_lt()\n",
    "                return torch._utils._rebuild_parameter(t, requires_grad, backward_hooks)\n",
    "\n",
    "            data._load_tensor = _load_tensor\n",
    "            return data\n",
    "        return torch._utils._rebuild_parameter(data, requires_grad, backward_hooks)\n",
    "\n",
    "    @classmethod\n",
    "    def rebuild_tensor_v2(\n",
    "        cls,\n",
    "        storage,\n",
    "        storage_offset,\n",
    "        size,\n",
    "        stride,\n",
    "        requires_grad,\n",
    "        backward_hooks,\n",
    "        metadata=None,\n",
    "        *,\n",
    "        archiveinfo=None,\n",
    "    ):\n",
    "        rebuild_args = (\n",
    "            storage_offset,\n",
    "            size,\n",
    "            stride,\n",
    "            requires_grad,\n",
    "            backward_hooks,\n",
    "            metadata,\n",
    "        )\n",
    "        metatensor = torch._utils._rebuild_tensor_v2(\n",
    "            storage,\n",
    "            storage_offset,\n",
    "            size,\n",
    "            stride,\n",
    "            requires_grad,\n",
    "            backward_hooks,\n",
    "            metadata,\n",
    "        )\n",
    "        storageinfo = storage.archiveinfo\n",
    "        return NotYetLoadedTensor(metatensor, archiveinfo, storageinfo, rebuild_args)\n",
    "\n",
    "    def _load_tensor(self):\n",
    "        name, storage_cls, fn, device, size = self.storageinfo\n",
    "        dtype = self.metatensor.dtype\n",
    "\n",
    "        uts = (\n",
    "            self.archiveinfo.zipfile_context.zf.get_storage_from_record(\n",
    "                f\"data/{fn}\",\n",
    "                size * torch._utils._element_size(dtype),\n",
    "                torch.UntypedStorage,\n",
    "            )\n",
    "            ._typed_storage()\n",
    "            ._untyped_storage\n",
    "        )\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            storage = torch.storage.TypedStorage(\n",
    "                wrap_storage=uts, dtype=self.metatensor.dtype, _internal=True\n",
    "            )\n",
    "        tensor = torch._utils._rebuild_tensor_v2(storage, *self.rebuild_args)\n",
    "        return tensor\n",
    "\n",
    "    @classmethod\n",
    "    def __torch_function__(cls, func, types, args=(), kwargs=None):\n",
    "        if kwargs is None:\n",
    "            kwargs = {}\n",
    "        loaded_args = [\n",
    "            (a._load_tensor() if isinstance(a, NotYetLoadedTensor) else a) for a in args\n",
    "        ]\n",
    "        res = func(*loaded_args, **kwargs)\n",
    "        # gc.collect would be costly here, maybe do it optionally\n",
    "        return res\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        # properties\n",
    "        ## TODO: device, is_...??\n",
    "        ## TODO: mH, mT, H, T, data, imag, real\n",
    "        ## name ???\n",
    "        if name in {\n",
    "            \"dtype\",\n",
    "            \"grad\",\n",
    "            \"grad_fn\",\n",
    "            \"layout\",\n",
    "            \"names\",\n",
    "            \"ndim\",\n",
    "            \"output_nr\",\n",
    "            \"requires_grad\",\n",
    "            \"retains_grad\",\n",
    "            \"shape\",\n",
    "            \"volatile\",\n",
    "        }:\n",
    "            return getattr(self.metatensor, name)\n",
    "        if name in {\"size\"}:\n",
    "            return getattr(self.metatensor, name)\n",
    "        # materializing with contiguous is needed for quantization\n",
    "        if name in {\"contiguous\"}:\n",
    "            return getattr(self._load_tensor(), name)\n",
    "\n",
    "        raise AttributeError(f\"{type(self)} does not have {name}\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"NotYetLoadedTensor({repr(self.metatensor)})\"\n",
    "    \n",
    "class LazyLoadingUnpickler(pickle.Unpickler):\n",
    "    def __init__(self, file, zipfile_context):\n",
    "        super().__init__(file)\n",
    "        self.zipfile_context = zipfile_context\n",
    "\n",
    "    def find_class(self, module, name):\n",
    "        res = super().find_class(module, name)\n",
    "        if module == \"torch._utils\" and name == \"_rebuild_tensor_v2\":\n",
    "            return functools.partial(\n",
    "                NotYetLoadedTensor.rebuild_tensor_v2, archiveinfo=self\n",
    "            )\n",
    "        elif module == \"torch._tensor\" and name == \"_rebuild_from_type_v2\":\n",
    "            return functools.partial(\n",
    "                NotYetLoadedTensor.rebuild_from_type_v2, archiveinfo=self\n",
    "            )\n",
    "        elif module == \"torch._utils\" and name == \"_rebuild_parameter\":\n",
    "            return functools.partial(\n",
    "                NotYetLoadedTensor.rebuild_parameter, archiveinfo=self\n",
    "            )\n",
    "        return res\n",
    "\n",
    "    def persistent_load(self, pid):\n",
    "        name, cls, fn, device, size = pid\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            s = torch.storage.TypedStorage(dtype=cls().dtype, device=\"meta\")\n",
    "        s.archiveinfo = pid\n",
    "        return s\n",
    "    \n",
    "class lazy_load:\n",
    "    def __init__(self, fn):\n",
    "        self.zf = torch._C.PyTorchFileReader(str(fn))\n",
    "        with BytesIO(self.zf.get_record(\"data.pkl\")) as pkl:\n",
    "            mup = LazyLoadingUnpickler(pkl, self)\n",
    "            self.sd = mup.load()\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self.sd\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        del self.zf  # I don't think there is a way to force closing...\n",
    "        self.zf = None\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model: LLaMA,\n",
    "    idx: torch.Tensor,\n",
    "    max_new_tokens: int,\n",
    "    *,\n",
    "    max_seq_length: Optional[int] = None,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: Optional[int] = None,\n",
    "    eos_id: Optional[int] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Takes a conditioning sequence (prompt) as input and continues to generate as many tokens as requested.\n",
    "\n",
    "    The implementation of this function is modified from A. Karpathy's nanoGPT.\n",
    "\n",
    "    Args:\n",
    "        model: The model to use.\n",
    "        idx: Tensor of shape (T) with indices of the prompt sequence.\n",
    "        max_new_tokens: The number of new tokens to generate.\n",
    "        max_seq_length: The maximum sequence length allowed.\n",
    "        temperature: Scales the predicted logits by 1 / temperature\n",
    "        top_k: If specified, only sample among the tokens with the k highest probabilities\n",
    "        eos_id: If specified, stop generating any more token once the <eos> token is triggered\n",
    "    \"\"\"\n",
    "    # create an empty tensor of the expected final shape and fill in the current tokens\n",
    "    T = idx.size(0)\n",
    "    T_new = T + max_new_tokens\n",
    "    if max_seq_length is None:\n",
    "        max_seq_length = min(T_new, model.config.block_size)\n",
    "\n",
    "    device, dtype = idx.device, idx.dtype\n",
    "    # create an empty tensor of the expected final shape and fill in the current tokens\n",
    "    empty = torch.empty(T_new, dtype=dtype, device=device)\n",
    "    empty[:T] = idx\n",
    "    idx = empty\n",
    "    input_pos = torch.arange(0, T, device=device)\n",
    "\n",
    "    # if idx.device.type == \"xla\":\n",
    "    #     import torch_xla.core.xla_model as xm\n",
    "\n",
    "    #     xm.mark_step()\n",
    "\n",
    "    # generate max_new_tokens tokens\n",
    "    for _ in range(max_new_tokens):\n",
    "        x = idx.index_select(0, input_pos).view(1, -1)\n",
    "\n",
    "        # forward\n",
    "        logits = model(x, max_seq_length, input_pos)\n",
    "        logits = logits[0, -1] / temperature\n",
    "\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits = torch.where(logits < v[[-1]], -float(\"Inf\"), logits)\n",
    "\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1).to(dtype=dtype)\n",
    "\n",
    "        # advance\n",
    "        input_pos = input_pos[-1:] + 1\n",
    "\n",
    "        # if idx.device.type == \"xla\":\n",
    "        #     xm.mark_step()\n",
    "\n",
    "        # concatenate the new generation\n",
    "        idx = idx.index_copy(0, input_pos, idx_next)\n",
    "\n",
    "        # if <eos> token is triggered, return the output (stop generation)\n",
    "        if idx_next == eos_id:\n",
    "            return idx[:input_pos]  # include the EOS token\n",
    "\n",
    "    return idx\n",
    "\n",
    "# The main function can be called directly in the notebook\n",
    "# Load pre-trained model and tokenizer\n",
    "checkpoint_path = \"out/training/iter-460000-ckpt.pth\"\n",
    "tokenizer_path = \"data/smiles/tokenizer.model\"\n",
    "\n",
    "assert Path(checkpoint_path).is_file(), checkpoint_path\n",
    "assert Path(tokenizer_path).is_file(), tokenizer_path\n",
    "\n",
    "print(\"Loading model ...\", file=sys.stderr)\n",
    "t0 = time.time()\n",
    "with lazy_load(checkpoint_path) as checkpoint:    \n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.to(device)\n",
    "print(f\"Time to load model: {time.time() - t0:.02f} seconds.\", file=sys.stderr)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C[C@H](C=NO)C(=O)N1CCN(S(N)(=O)=O)CC1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 1: 3.97 sec total, 7.55 tokens/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cc1cccc(S(=O)(=O)NCCCO[C@H]2CCCCO2)c1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 2: 1.72 sec total, 16.86 tokens/sec\n",
      "Memory used: 2.06 GB\n"
     ]
    }
   ],
   "source": [
    "prompt = \"C\"\n",
    "num_samples = 2\n",
    "max_new_tokens = 100\n",
    "top_k = 200\n",
    "temperature = 1\n",
    "\n",
    "model.eval()\n",
    "tokenizer = Tokenizer(tokenizer_path)\n",
    "\n",
    "encoded = tokenizer.encode(prompt, bos=True, eos=False, device=device)\n",
    "prompt_length = encoded.size(0)\n",
    "\n",
    "# L.seed_everything(1234)\n",
    "for i in range(num_samples):\n",
    "    t0 = time.perf_counter()\n",
    "    y = generate(model, encoded, max_new_tokens, temperature=temperature, top_k=top_k, eos_id=tokenizer.eos_id)\n",
    "    t = time.perf_counter() - t0\n",
    "\n",
    "    model.reset_cache()\n",
    "    smiles_string = tokenizer.decode(y)\n",
    "    print(smiles_string)\n",
    "    tokens_generated = y.size(0) - prompt_length\n",
    "    print(f\"Time for inference {i + 1}: {t:.02f} sec total, {tokens_generated / t:.02f} tokens/sec\", file=sys.stderr)\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cc1cccc(S(=O)(=O)NCCCO[C@H]2CCCCO2)c1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit import RDLogger\n",
    "\n",
    "# Suppress RDKit error messages\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "mol = None\n",
    "\n",
    "try:\n",
    "    mol = Chem.MolFromSmiles(smiles_string)\n",
    "    \n",
    "except Exception as e:\n",
    "    # Optionally, log any unexpected errors\n",
    "    print(f\"Error: {e}\")\n",
    "    \n",
    "\n",
    "# Draw the valid molecule\n",
    "Draw.MolToMPL(mol, size=(200,200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/3dmoljs_load.v0": "<div id=\"3dmolviewer_16966045766636336\"  style=\"position: relative; width: 400px; height: 400px\">\n        <p id=\"3dmolwarning_16966045766636336\" style=\"background-color:#ffcccc;color:black\">You appear to be running in JupyterLab (or JavaScript failed to load for some other reason).  You need to install the 3dmol extension: <br>\n        <tt>jupyter labextension install jupyterlab_3dmol</tt></p>\n        </div>\n<script>\n\nvar loadScriptAsync = function(uri){\n  return new Promise((resolve, reject) => {\n    //this is to ignore the existence of requirejs amd\n    var savedexports, savedmodule;\n    if (typeof exports !== 'undefined') savedexports = exports;\n    else exports = {}\n    if (typeof module !== 'undefined') savedmodule = module;\n    else module = {}\n\n    var tag = document.createElement('script');\n    tag.src = uri;\n    tag.async = true;\n    tag.onload = () => {\n        exports = savedexports;\n        module = savedmodule;\n        resolve();\n    };\n  var firstScriptTag = document.getElementsByTagName('script')[0];\n  firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);\n});\n};\n\nif(typeof $3Dmolpromise === 'undefined') {\n$3Dmolpromise = null;\n  $3Dmolpromise = loadScriptAsync('https://cdnjs.cloudflare.com/ajax/libs/3Dmol/2.0.1/3Dmol-min.js');\n}\n\nvar viewer_16966045766636336 = null;\nvar warn = document.getElementById(\"3dmolwarning_16966045766636336\");\nif(warn) {\n    warn.parentNode.removeChild(warn);\n}\n$3Dmolpromise.then(function() {\nviewer_16966045766636336 = $3Dmol.createViewer(document.getElementById(\"3dmolviewer_16966045766636336\"),{backgroundColor:\"white\"});\nviewer_16966045766636336.zoomTo();\n\tviewer_16966045766636336.addModel(\"\\n     RDKit          3D\\n\\n 44 45  0  0  0  0  0  0  0  0999 V2000\\n    3.0494   -3.2719   -0.5198 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    3.5024   -1.8421   -0.4810 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    4.6764   -1.4868    0.1975 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    5.0936   -0.1542    0.2526 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    4.3320    0.8422   -0.3550 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    3.1452    0.4957   -1.0055 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    2.1735    1.7655   -1.7801 S   0  0  0  0  0  6  0  0  0  0  0  0\\n    1.2570    1.1349   -2.7029 O   0  0  0  0  0  0  0  0  0  0  0  0\\n    3.0736    2.8179   -2.1986 O   0  0  0  0  0  0  0  0  0  0  0  0\\n    1.2915    2.3870   -0.4924 N   0  0  0  0  0  0  0  0  0  0  0  0\\n    0.4359    1.4926    0.2946 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -0.6825    2.2951    0.9535 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -1.7996    2.6708   -0.0190 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -2.4054    1.5593   -0.6741 O   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.2132    0.7395    0.1663 C   0  0  1  0  0  0  0  0  0  0  0  0\\n   -4.2807    0.0715   -0.7147 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -4.7036   -1.2700   -0.1400 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.4931   -2.1892    0.0142 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -2.1911   -1.4050    0.1433 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -2.4157   -0.2094    0.8820 O   0  0  0  0  0  0  0  0  0  0  0  0\\n    2.7310   -0.8388   -1.0796 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    2.4995   -3.4820   -1.4432 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    3.9053   -3.9543   -0.4894 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    2.3992   -3.4812    0.3352 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    5.2817   -2.2515    0.6807 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    6.0165    0.1037    0.7668 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    4.6668    1.8762   -0.3204 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    1.7623    3.1544   -0.0173 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    1.0511    1.0403    1.0803 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    0.0278    0.6888   -0.3236 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -0.2787    3.2097    1.4050 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -1.0983    1.7014    1.7742 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -2.5702    3.2577    0.4946 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -1.3923    3.3155   -0.8057 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.7213    1.3582    0.9159 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -5.1413    0.7419   -0.8229 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.8948   -0.0754   -1.7325 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -5.4594   -1.7448   -0.7750 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -5.1687   -1.1113    0.8410 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.4170   -2.8781   -0.8348 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.6427   -2.8105    0.9055 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -1.7687   -1.1623   -0.8393 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -1.4451   -2.0043    0.6751 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    1.8118   -1.0965   -1.6030 H   0  0  0  0  0  0  0  0  0  0  0  0\\n  1  2  1  0\\n  2  3  2  0\\n  3  4  1  0\\n  4  5  2  0\\n  5  6  1  0\\n  6  7  1  0\\n  7  8  2  0\\n  7  9  2  0\\n  7 10  1  0\\n 10 11  1  0\\n 11 12  1  0\\n 12 13  1  0\\n 13 14  1  0\\n 14 15  1  0\\n 15 16  1  0\\n 16 17  1  0\\n 17 18  1  0\\n 18 19  1  0\\n 19 20  1  0\\n  6 21  2  0\\n 21  2  1  0\\n 20 15  1  0\\n  1 22  1  0\\n  1 23  1  0\\n  1 24  1  0\\n  3 25  1  0\\n  4 26  1  0\\n  5 27  1  0\\n 10 28  1  0\\n 11 29  1  0\\n 11 30  1  0\\n 12 31  1  0\\n 12 32  1  0\\n 13 33  1  0\\n 13 34  1  0\\n 15 35  1  1\\n 16 36  1  0\\n 16 37  1  0\\n 17 38  1  0\\n 17 39  1  0\\n 18 40  1  0\\n 18 41  1  0\\n 19 42  1  0\\n 19 43  1  0\\n 21 44  1  0\\nM  END\\n\");\n\tviewer_16966045766636336.setStyle({\"stick\": {}});\n\tviewer_16966045766636336.zoomTo();\nviewer_16966045766636336.render();\n});\n</script>",
      "text/html": [
       "<div id=\"3dmolviewer_16966045766636336\"  style=\"position: relative; width: 400px; height: 400px\">\n",
       "        <p id=\"3dmolwarning_16966045766636336\" style=\"background-color:#ffcccc;color:black\">You appear to be running in JupyterLab (or JavaScript failed to load for some other reason).  You need to install the 3dmol extension: <br>\n",
       "        <tt>jupyter labextension install jupyterlab_3dmol</tt></p>\n",
       "        </div>\n",
       "<script>\n",
       "\n",
       "var loadScriptAsync = function(uri){\n",
       "  return new Promise((resolve, reject) => {\n",
       "    //this is to ignore the existence of requirejs amd\n",
       "    var savedexports, savedmodule;\n",
       "    if (typeof exports !== 'undefined') savedexports = exports;\n",
       "    else exports = {}\n",
       "    if (typeof module !== 'undefined') savedmodule = module;\n",
       "    else module = {}\n",
       "\n",
       "    var tag = document.createElement('script');\n",
       "    tag.src = uri;\n",
       "    tag.async = true;\n",
       "    tag.onload = () => {\n",
       "        exports = savedexports;\n",
       "        module = savedmodule;\n",
       "        resolve();\n",
       "    };\n",
       "  var firstScriptTag = document.getElementsByTagName('script')[0];\n",
       "  firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);\n",
       "});\n",
       "};\n",
       "\n",
       "if(typeof $3Dmolpromise === 'undefined') {\n",
       "$3Dmolpromise = null;\n",
       "  $3Dmolpromise = loadScriptAsync('https://cdnjs.cloudflare.com/ajax/libs/3Dmol/2.0.1/3Dmol-min.js');\n",
       "}\n",
       "\n",
       "var viewer_16966045766636336 = null;\n",
       "var warn = document.getElementById(\"3dmolwarning_16966045766636336\");\n",
       "if(warn) {\n",
       "    warn.parentNode.removeChild(warn);\n",
       "}\n",
       "$3Dmolpromise.then(function() {\n",
       "viewer_16966045766636336 = $3Dmol.createViewer(document.getElementById(\"3dmolviewer_16966045766636336\"),{backgroundColor:\"white\"});\n",
       "viewer_16966045766636336.zoomTo();\n",
       "\tviewer_16966045766636336.addModel(\"\\n     RDKit          3D\\n\\n 44 45  0  0  0  0  0  0  0  0999 V2000\\n    3.0494   -3.2719   -0.5198 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    3.5024   -1.8421   -0.4810 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    4.6764   -1.4868    0.1975 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    5.0936   -0.1542    0.2526 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    4.3320    0.8422   -0.3550 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    3.1452    0.4957   -1.0055 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    2.1735    1.7655   -1.7801 S   0  0  0  0  0  6  0  0  0  0  0  0\\n    1.2570    1.1349   -2.7029 O   0  0  0  0  0  0  0  0  0  0  0  0\\n    3.0736    2.8179   -2.1986 O   0  0  0  0  0  0  0  0  0  0  0  0\\n    1.2915    2.3870   -0.4924 N   0  0  0  0  0  0  0  0  0  0  0  0\\n    0.4359    1.4926    0.2946 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -0.6825    2.2951    0.9535 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -1.7996    2.6708   -0.0190 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -2.4054    1.5593   -0.6741 O   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.2132    0.7395    0.1663 C   0  0  1  0  0  0  0  0  0  0  0  0\\n   -4.2807    0.0715   -0.7147 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -4.7036   -1.2700   -0.1400 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.4931   -2.1892    0.0142 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -2.1911   -1.4050    0.1433 C   0  0  0  0  0  0  0  0  0  0  0  0\\n   -2.4157   -0.2094    0.8820 O   0  0  0  0  0  0  0  0  0  0  0  0\\n    2.7310   -0.8388   -1.0796 C   0  0  0  0  0  0  0  0  0  0  0  0\\n    2.4995   -3.4820   -1.4432 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    3.9053   -3.9543   -0.4894 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    2.3992   -3.4812    0.3352 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    5.2817   -2.2515    0.6807 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    6.0165    0.1037    0.7668 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    4.6668    1.8762   -0.3204 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    1.7623    3.1544   -0.0173 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    1.0511    1.0403    1.0803 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    0.0278    0.6888   -0.3236 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -0.2787    3.2097    1.4050 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -1.0983    1.7014    1.7742 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -2.5702    3.2577    0.4946 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -1.3923    3.3155   -0.8057 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.7213    1.3582    0.9159 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -5.1413    0.7419   -0.8229 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.8948   -0.0754   -1.7325 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -5.4594   -1.7448   -0.7750 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -5.1687   -1.1113    0.8410 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.4170   -2.8781   -0.8348 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -3.6427   -2.8105    0.9055 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -1.7687   -1.1623   -0.8393 H   0  0  0  0  0  0  0  0  0  0  0  0\\n   -1.4451   -2.0043    0.6751 H   0  0  0  0  0  0  0  0  0  0  0  0\\n    1.8118   -1.0965   -1.6030 H   0  0  0  0  0  0  0  0  0  0  0  0\\n  1  2  1  0\\n  2  3  2  0\\n  3  4  1  0\\n  4  5  2  0\\n  5  6  1  0\\n  6  7  1  0\\n  7  8  2  0\\n  7  9  2  0\\n  7 10  1  0\\n 10 11  1  0\\n 11 12  1  0\\n 12 13  1  0\\n 13 14  1  0\\n 14 15  1  0\\n 15 16  1  0\\n 16 17  1  0\\n 17 18  1  0\\n 18 19  1  0\\n 19 20  1  0\\n  6 21  2  0\\n 21  2  1  0\\n 20 15  1  0\\n  1 22  1  0\\n  1 23  1  0\\n  1 24  1  0\\n  3 25  1  0\\n  4 26  1  0\\n  5 27  1  0\\n 10 28  1  0\\n 11 29  1  0\\n 11 30  1  0\\n 12 31  1  0\\n 12 32  1  0\\n 13 33  1  0\\n 13 34  1  0\\n 15 35  1  1\\n 16 36  1  0\\n 16 37  1  0\\n 17 38  1  0\\n 17 39  1  0\\n 18 40  1  0\\n 18 41  1  0\\n 19 42  1  0\\n 19 43  1  0\\n 21 44  1  0\\nM  END\\n\");\n",
       "\tviewer_16966045766636336.setStyle({\"stick\": {}});\n",
       "\tviewer_16966045766636336.zoomTo();\n",
       "viewer_16966045766636336.render();\n",
       "});\n",
       "</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import py3Dmol\n",
    "\n",
    "def show_molecule_3d(mol):\n",
    "    mb = Chem.MolToMolBlock(mol)\n",
    "    viewer = py3Dmol.view(width=400, height=400)\n",
    "    viewer.addModel(mb, format=\"mol\")\n",
    "    viewer.setStyle({'stick': {}})\n",
    "    viewer.zoomTo()\n",
    "    return viewer.show()\n",
    "    \n",
    "# Add Hydrogens to the molecule\n",
    "mol = Chem.AddHs(mol)\n",
    "\n",
    "# Generate 3D coordinates\n",
    "AllChem.EmbedMolecule(mol, AllChem.ETKDG())\n",
    "\n",
    "# Optimize the molecule using a force field (e.g., MMFF94)\n",
    "AllChem.MMFFOptimizeMolecule(mol, maxIters=500, nonBondedThresh=500.0)\n",
    "\n",
    "# Visualize\n",
    "show_molecule_3d(mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 50 molecules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 50: 2.32 sec total, 12.52 tokens/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 100 molecules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 100: 3.19 sec total, 11.60 tokens/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 150 molecules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 150: 2.91 sec total, 13.39 tokens/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 200 molecules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 200: 3.70 sec total, 11.09 tokens/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 250 molecules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 250: 2.42 sec total, 12.80 tokens/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 300 molecules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 300: 3.18 sec total, 13.84 tokens/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 350 molecules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 350: 3.49 sec total, 13.19 tokens/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 400 molecules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 400: 2.72 sec total, 10.28 tokens/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 450 molecules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 450: 3.02 sec total, 11.27 tokens/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 500 molecules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 500: 2.60 sec total, 12.30 tokens/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 550 molecules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 550: 2.36 sec total, 14.84 tokens/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 600 molecules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 600: 1.65 sec total, 16.93 tokens/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 650 molecules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 650: 1.75 sec total, 18.34 tokens/sec\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Out of range: piece id is out of range.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects\\LLaMA-Mol\\3_mol_gen_pre-trained.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/3_mol_gen_pre-trained.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m t \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter() \u001b[39m-\u001b[39m t0\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/3_mol_gen_pre-trained.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m model\u001b[39m.\u001b[39mreset_cache()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/3_mol_gen_pre-trained.ipynb#X13sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m smiles_string \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/3_mol_gen_pre-trained.ipynb#X13sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m all_smiles\u001b[39m.\u001b[39mappend(smiles_string)   \u001b[39m# Append the generated molecule to the list\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/3_mol_gen_pre-trained.ipynb#X13sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Print progress after every 1000 molecules\u001b[39;00m\n",
      "\u001b[1;32md:\\Projects\\LLaMA-Mol\\3_mol_gen_pre-trained.ipynb Cell 11\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/3_mol_gen_pre-trained.ipynb#X13sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, tokens: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/3_mol_gen_pre-trained.ipynb#X13sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessor\u001b[39m.\u001b[39mdecode(tokens\u001b[39m.\u001b[39mtolist())\n",
      "File \u001b[1;32mc:\\Users\\m1000\\anaconda3\\envs\\my-rdkit-env\\Lib\\site-packages\\sentencepiece\\__init__.py:780\u001b[0m, in \u001b[0;36mSentencePieceProcessor.Decode\u001b[1;34m(self, input, out_type, num_threads)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39minput\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mlist\u001b[39m:\n\u001b[0;32m    779\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39minput\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39minput\u001b[39m[\u001b[39m0\u001b[39m]) \u001b[39mis\u001b[39;00m \u001b[39mint\u001b[39m:\n\u001b[1;32m--> 780\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_DecodeIds(\u001b[39minput\u001b[39m)\n\u001b[0;32m    781\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39minput\u001b[39m[\u001b[39m0\u001b[39m]) \u001b[39mis\u001b[39;00m \u001b[39mstr\u001b[39m:\n\u001b[0;32m    782\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_DecodePieces(\u001b[39minput\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\m1000\\anaconda3\\envs\\my-rdkit-env\\Lib\\site-packages\\sentencepiece\\__init__.py:337\u001b[0m, in \u001b[0;36mSentencePieceProcessor._DecodeIds\u001b[1;34m(self, ids)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_DecodeIds\u001b[39m(\u001b[39mself\u001b[39m, ids):\n\u001b[1;32m--> 337\u001b[0m     \u001b[39mreturn\u001b[39;00m _sentencepiece\u001b[39m.\u001b[39mSentencePieceProcessor__DecodeIds(\u001b[39mself\u001b[39m, ids)\n",
      "\u001b[1;31mIndexError\u001b[0m: Out of range: piece id is out of range."
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "prompt = \"C\"\n",
    "num_samples = 100000\n",
    "max_new_tokens = 100\n",
    "top_k = 200\n",
    "temperature = 1\n",
    "\n",
    "model.eval()\n",
    "tokenizer = Tokenizer(tokenizer_path)\n",
    "\n",
    "encoded = tokenizer.encode(prompt, bos=True, eos=False, device=device)\n",
    "prompt_length = encoded.size(0)\n",
    "\n",
    "all_smiles = []\n",
    "\n",
    "# Create or overwrite the CSV file and write the header\n",
    "with open(\"generated_molecules.csv\", \"w\", newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"Molecule\"])\n",
    "\n",
    "for i in range(num_samples):\n",
    "    t0 = time.perf_counter()\n",
    "    y = generate(model, encoded, max_new_tokens, temperature=temperature, top_k=top_k, eos_id=tokenizer.eos_id)\n",
    "    t = time.perf_counter() - t0\n",
    "\n",
    "    model.reset_cache()\n",
    "    smiles_string = tokenizer.decode(y)\n",
    "    all_smiles.append(smiles_string)\n",
    "\n",
    "    # After every 1000 molecules, write to CSV and clear the list\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        with open(\"generated_molecules.csv\", \"a\", newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            for smiles in all_smiles:\n",
    "                writer.writerow([smiles])\n",
    "        all_smiles.clear()\n",
    "\n",
    "        print(f\"Generated {i + 1} molecules\")\n",
    "        tokens_generated = y.size(0) - prompt_length\n",
    "        print(f\"Time for inference {i + 1}: {t:.02f} sec total, {tokens_generated / t:.02f} tokens/sec\", file=sys.stderr)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\", file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"generated_molecules.txt\", \"w\") as f:\n",
    "    for smiles in all_smiles:\n",
    "        f.write(smiles + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-rdkit-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
