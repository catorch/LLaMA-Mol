{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from typing_extensions import Self\n",
    "from lightning.fabric.strategies import DeepSpeedStrategy, FSDPStrategy\n",
    "from torch import distributed as dist\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaskCache = torch.Tensor\n",
    "RoPECache = torch.Tensor\n",
    "KVCache = Tuple[torch.Tensor, torch.Tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_multiple(n: int, k: int) -> int:\n",
    "    if n % k == 0:\n",
    "        return n\n",
    "    return n + k - (n % k)\n",
    "\n",
    "def save_model_checkpoint(model, file_path):\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    # Check if distributed training is initialized, if not, save the model directly\n",
    "    if not dist.is_initialized() or dist.get_rank() == 0:\n",
    "        state_dict = model.state_dict()\n",
    "        torch.save(state_dict, file_path)\n",
    "\n",
    "    # If distributed training is initialized, do a synchronization barrier after saving the model\n",
    "    if dist.is_initialized():\n",
    "        dist.barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_configs = {\n",
    "    \"7B\": dict(n_layer=32, n_head=32, n_embd=1024),\n",
    "    \"13B\": dict(n_layer=40, n_head=40, n_embd=5120),\n",
    "    \"30B\": dict(n_layer=60, n_head=52, n_embd=6656),\n",
    "    \"65B\": dict(n_layer=80, n_head=64, n_embd=8192),\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LLaMAConfig:\n",
    "    block_size: int = 2048\n",
    "    vocab_size: int = 32000\n",
    "    padded_vocab_size: Optional[int] = None\n",
    "    n_layer: int = 32\n",
    "    n_head: int = 32\n",
    "    n_embd: int = 4096\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.padded_vocab_size is None:\n",
    "            self.padded_vocab_size = find_multiple(self.vocab_size, 64)\n",
    "\n",
    "    @classmethod\n",
    "    def from_name(cls, name: str) -> Self:\n",
    "        return cls(**llama_configs[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLaMA(nn.Module):\n",
    "    def __init__(self, config: LLaMAConfig) -> None:\n",
    "        super().__init__()\n",
    "        assert config.padded_vocab_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.padded_vocab_size, bias=False)\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte=nn.Embedding(config.padded_vocab_size, config.n_embd),\n",
    "                h=nn.ModuleList(Block(config) for _ in range(config.n_layer)),\n",
    "                ln_f=RMSNorm(config.n_embd),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.rope_cache: Optional[RoPECache] = None\n",
    "        self.mask_cache: Optional[MaskCache] = None\n",
    "        self.kv_caches: List[KVCache] = []\n",
    "\n",
    "    def _init_weights(self, module: nn.Module) -> None:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layer))\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layer))\n",
    "\n",
    "    def forward(\n",
    "        self, idx: torch.Tensor, max_seq_length: Optional[int] = None, input_pos: Optional[torch.Tensor] = None\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, List[KVCache]]]:\n",
    "        B, T = idx.size()\n",
    "\n",
    "        block_size = self.config.block_size\n",
    "        if max_seq_length is None:\n",
    "            max_seq_length = block_size\n",
    "        assert T <= max_seq_length, f\"Cannot forward sequence of length {T}, max seq length is only {max_seq_length}\"\n",
    "        assert max_seq_length <= block_size, f\"Cannot attend to {max_seq_length}, block size is only {block_size}\"\n",
    "        assert T <= block_size, f\"Cannot forward sequence of length {T}, block size is only {block_size}\"\n",
    "\n",
    "        if self.rope_cache is None:\n",
    "            self.rope_cache = self.build_rope_cache(idx)\n",
    "        if self.mask_cache is None:\n",
    "            self.mask_cache = self.build_mask_cache(idx)\n",
    "\n",
    "        if input_pos is not None:\n",
    "            rope = self.rope_cache.index_select(0, input_pos)\n",
    "            mask = self.mask_cache.index_select(2, input_pos)\n",
    "            mask = mask[:, :, :, :max_seq_length]\n",
    "        else:\n",
    "            rope = self.rope_cache[:T]\n",
    "            mask = self.mask_cache[:, :, :T, :T]\n",
    "\n",
    "        # forward the model itself\n",
    "        x = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n",
    "\n",
    "        if input_pos is None:  # proxy for use_cache=False\n",
    "            for block in self.transformer.h:\n",
    "                x, _ = block(x, rope, mask, max_seq_length)\n",
    "        else:\n",
    "            if not self.kv_caches:\n",
    "                head_size = self.config.n_embd // self.config.n_head\n",
    "                cache_shape = (B, self.config.n_head, max_seq_length, head_size)\n",
    "                self.kv_caches = [\n",
    "                    (torch.zeros(cache_shape, device=x.device, dtype=x.dtype), torch.zeros(cache_shape, device=x.device, dtype=x.dtype))\n",
    "                    for _ in range(self.config.n_layer)\n",
    "                ]\n",
    "            for i, block in enumerate(self.transformer.h):\n",
    "                x, self.kv_caches[i] = block(x, rope, mask, max_seq_length, input_pos, self.kv_caches[i])\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        logits = self.lm_head(x)  # (b, t, vocab_size)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    @classmethod\n",
    "    def from_name(cls, name: str) -> Self:\n",
    "        return cls(LLaMAConfig.from_name(name))\n",
    "\n",
    "    def build_rope_cache(self, idx: torch.Tensor) -> RoPECache:\n",
    "        return build_rope_cache(\n",
    "            seq_len=self.config.block_size,\n",
    "            n_elem=self.config.n_embd // self.config.n_head,\n",
    "            dtype=idx.dtype,\n",
    "            device=idx.device,\n",
    "        )\n",
    "\n",
    "    def build_mask_cache(self, idx: torch.Tensor) -> MaskCache:\n",
    "        ones = torch.ones((self.config.block_size, self.config.block_size), device=idx.device, dtype=torch.bool)\n",
    "        return torch.tril(ones).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    def reset_cache(self) -> None:\n",
    "        self.kv_caches.clear()\n",
    "        if self.mask_cache.device.type == \"xla\":\n",
    "            # https://github.com/Lightning-AI/lit-parrot/pull/83#issuecomment-1558150179\n",
    "            self.rope_cache = None\n",
    "            self.mask_cache = None\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config: LLaMAConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.rms_1 = RMSNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.rms_2 = RMSNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        rope: RoPECache,\n",
    "        mask: MaskCache,\n",
    "        max_seq_length: int,\n",
    "        input_pos: Optional[torch.Tensor] = None,\n",
    "        kv_cache: Optional[KVCache] = None,\n",
    "    ) -> Tuple[torch.Tensor, Optional[KVCache]]:\n",
    "        h, new_kv_cache = self.attn(self.rms_1(x), rope, mask, max_seq_length, input_pos, kv_cache)\n",
    "        x = x + h\n",
    "        x = x + self.mlp(self.rms_2(x))\n",
    "        return x, new_kv_cache\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config: LLaMAConfig) -> None:\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.block_size = config.block_size\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        rope: RoPECache,\n",
    "        mask: MaskCache,\n",
    "        max_seq_length: int,\n",
    "        input_pos: Optional[torch.Tensor] = None,\n",
    "        kv_cache: Optional[KVCache] = None,\n",
    "    ) -> Tuple[torch.Tensor, Optional[KVCache]]:\n",
    "        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "\n",
    "        head_size = C // self.n_head\n",
    "        k = k.view(B, T, self.n_head, head_size)\n",
    "        q = q.view(B, T, self.n_head, head_size)\n",
    "        v = v.view(B, T, self.n_head, head_size)\n",
    "\n",
    "        q = apply_rope(q, rope)\n",
    "        k = apply_rope(k, rope)\n",
    "\n",
    "        k = k.transpose(1, 2)  # (B, nh, T, hs)\n",
    "        q = q.transpose(1, 2)  # (B, nh, T, hs)\n",
    "        v = v.transpose(1, 2)  # (B, nh, T, hs)\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            cache_k, cache_v = kv_cache\n",
    "            # check if reached token limit\n",
    "            if input_pos[-1] >= max_seq_length:\n",
    "                input_pos = torch.tensor(max_seq_length - 1, device=input_pos.device)\n",
    "                # shift 1 position to the left\n",
    "                cache_k = torch.roll(cache_k, -1, dims=2)\n",
    "                cache_v = torch.roll(cache_v, -1, dims=2)\n",
    "            k = cache_k.index_copy(2, input_pos, k)\n",
    "            v = cache_v.index_copy(2, input_pos, v)\n",
    "            kv_cache = k, v\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        #  att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        #  att = att.masked_fill(mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "        #  att = F.softmax(att, dim=-1)\n",
    "        #  y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        # efficient attention using Flash Attention CUDA kernels\n",
    "        y = F.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=0.0)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "\n",
    "        return y, kv_cache\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: LLaMAConfig) -> None:\n",
    "        super().__init__()\n",
    "        hidden_dim = 4 * config.n_embd\n",
    "        n_hidden = int(2 * hidden_dim / 3)\n",
    "        n_hidden = find_multiple(n_hidden, 256)\n",
    "\n",
    "        self.c_fc1 = nn.Linear(config.n_embd, n_hidden, bias=False)\n",
    "        self.c_fc2 = nn.Linear(config.n_embd, n_hidden, bias=False)\n",
    "        self.c_proj = nn.Linear(n_hidden, config.n_embd, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.silu(self.c_fc1(x)) * self.c_fc2(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization.\n",
    "\n",
    "    Derived from https://github.com/bzhangGo/rmsnorm/blob/master/rmsnorm_torch.py. BSD 3-Clause License:\n",
    "    https://github.com/bzhangGo/rmsnorm/blob/master/LICENSE.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size: int, dim: int = -1, eps: float = 1e-5) -> None:\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(size))\n",
    "        self.eps = eps\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # NOTE: the original RMSNorm paper implementation is not equivalent\n",
    "        # norm_x = x.norm(2, dim=self.dim, keepdim=True)\n",
    "        # rms_x = norm_x * d_x ** (-1. / 2)\n",
    "        # x_normed = x / (rms_x + self.eps)\n",
    "        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)\n",
    "        x_normed = x * torch.rsqrt(norm_x + self.eps)\n",
    "        return self.scale * x_normed\n",
    "\n",
    "\n",
    "def build_rope_cache(\n",
    "    seq_len: int, n_elem: int, dtype: torch.dtype, device: torch.device, base: int = 10000\n",
    ") -> RoPECache:\n",
    "    \"\"\"Enhanced Transformer with Rotary Position Embedding.\n",
    "\n",
    "    Derived from: https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/\n",
    "    transformers/rope/__init__.py. MIT License:\n",
    "    https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/license.\n",
    "    \"\"\"\n",
    "    # $\\Theta = {\\theta_i = 10000^{\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n",
    "    theta = 1.0 / (base ** (torch.arange(0, n_elem, 2, dtype=dtype, device=device) / n_elem))\n",
    "\n",
    "    # Create position indexes `[0, 1, ..., seq_len - 1]`\n",
    "    seq_idx = torch.arange(seq_len, dtype=dtype, device=device)\n",
    "\n",
    "    # Calculate the product of position index and $\\theta_i$\n",
    "    idx_theta = torch.outer(seq_idx, theta).float()\n",
    "\n",
    "    cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)\n",
    "\n",
    "    # this is to mimic the behaviour of complex32, else we will get different results\n",
    "    if dtype in (torch.float16, torch.bfloat16, torch.int8):\n",
    "        cache = cache.half()\n",
    "    return cache\n",
    "\n",
    "\n",
    "def apply_rope(x: torch.Tensor, rope_cache: RoPECache) -> torch.Tensor:\n",
    "    # truncate to support variable sizes\n",
    "    T = x.size(1)\n",
    "    rope_cache = rope_cache[:T]\n",
    "\n",
    "    # cast because the reference does\n",
    "    xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n",
    "    rope_cache = rope_cache.view(1, xshaped.size(1), 1, xshaped.size(3), 2)\n",
    "    x_out2 = torch.stack(\n",
    "        [\n",
    "            xshaped[..., 0] * rope_cache[..., 0] - xshaped[..., 1] * rope_cache[..., 1],\n",
    "            xshaped[..., 1] * rope_cache[..., 0] + xshaped[..., 0] * rope_cache[..., 1],\n",
    "        ],\n",
    "        -1,\n",
    "    )\n",
    "\n",
    "    x_out2 = x_out2.flatten(3)\n",
    "    return x_out2.type_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from sentencepiece import SentencePieceProcessor, SentencePieceTrainer\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"Tokenizer for LLaMA.\"\"\"\n",
    "\n",
    "    def __init__(self, model_path: Path) -> None:\n",
    "        self.processor = SentencePieceProcessor(model_file=str(model_path))\n",
    "        self.bos_id = self.processor.bos_id()\n",
    "        self.eos_id = self.processor.eos_id()\n",
    "        self.pad_id = self.processor.pad_id()\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return self.processor.vocab_size()\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        string: str,\n",
    "        bos: bool = True,\n",
    "        eos: bool = True,\n",
    "        max_length: int = -1,\n",
    "        pad: bool = False,\n",
    "        device: Optional[torch.device] = None\n",
    "    ) -> torch.Tensor:\n",
    "        lines = string.splitlines()\n",
    "        all_tokens = []\n",
    "        for line in lines:\n",
    "            tokens = self.processor.encode(line)\n",
    "            if bos:\n",
    "                tokens = [self.bos_id] + tokens\n",
    "            if eos:\n",
    "                tokens = tokens + [self.eos_id]\n",
    "            if max_length > 0:\n",
    "                tokens = tokens[:max_length]\n",
    "            if pad and len(tokens) < max_length:\n",
    "                tokens += [self.pad_id] * (max_length - len(tokens))\n",
    "            all_tokens.extend(tokens)\n",
    "        return torch.tensor(all_tokens, dtype=torch.int, device=device)\n",
    "\n",
    "    def decode(self, tokens: torch.Tensor) -> str:\n",
    "        return self.processor.decode(tokens.tolist())\n",
    "    \n",
    "    @staticmethod\n",
    "    def train(input: str, destination: str, vocab_size=32000) -> None:\n",
    "        model_prefix = os.path.join(destination, \"tokenizer\")\n",
    "        SentencePieceTrainer.Train(input=input, model_prefix=model_prefix, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "block_size = 1024\n",
    "config = LLaMAConfig.from_name(\"7B\")\n",
    "config.block_size = block_size\n",
    "config.vocab_size = 100  # from prepare_shakespeare.py\n",
    "\n",
    "# Model initialization\n",
    "model = LLaMA(config)\n",
    "# model.load_state_dict(torch.load(\"./out/training/iter-460000-ckpt.pth\"))\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model ...\n",
      "Time to load model: 2.64 seconds.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import torch\n",
    "import lightning as L\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "import functools\n",
    "import pickle\n",
    "\n",
    "def llama_model_lookup(checkpoint: dict) -> str:\n",
    "    \"\"\"Returns the LLaMA model name from the checkpoint.\n",
    "    \n",
    "    Checks the width of the lm_head.weight matrix, as these uniquely identify the model.\n",
    "    \"\"\"\n",
    "    embedding_size = checkpoint['transformer.wte.weight'].shape[1]\n",
    "    return llama_model_sizes[embedding_size]\n",
    "class NotYetLoadedTensor:\n",
    "    def __init__(self, metatensor, archiveinfo, storageinfo, rebuild_args):\n",
    "        self.metatensor = metatensor\n",
    "        self.archiveinfo = archiveinfo\n",
    "        self.storageinfo = storageinfo\n",
    "        self.rebuild_args = rebuild_args\n",
    "\n",
    "    @classmethod\n",
    "    def rebuild_from_type_v2(cls, func, new_type, args, state, *, archiveinfo=None):\n",
    "        ret = func(*args)\n",
    "        if isinstance(ret, NotYetLoadedTensor):\n",
    "            old_lt = ret._load_tensor\n",
    "\n",
    "            def _load_tensor():\n",
    "                t = old_lt()\n",
    "                return torch._tensor._rebuild_from_type_v2(\n",
    "                    lambda: t, new_type, (), state\n",
    "                )\n",
    "\n",
    "            ret._load_tensor = _load_tensor\n",
    "            return ret\n",
    "        return torch._tensor._rebuild_from_type_v2(func, new_type, args, state)\n",
    "\n",
    "    @classmethod\n",
    "    def rebuild_parameter(\n",
    "        cls, data, requires_grad, backward_hooks, *, archiveinfo=None\n",
    "    ):\n",
    "        if isinstance(data, NotYetLoadedTensor):\n",
    "            old_lt = data._load_tensor\n",
    "\n",
    "            def _load_tensor():\n",
    "                t = old_lt()\n",
    "                return torch._utils._rebuild_parameter(t, requires_grad, backward_hooks)\n",
    "\n",
    "            data._load_tensor = _load_tensor\n",
    "            return data\n",
    "        return torch._utils._rebuild_parameter(data, requires_grad, backward_hooks)\n",
    "\n",
    "    @classmethod\n",
    "    def rebuild_tensor_v2(\n",
    "        cls,\n",
    "        storage,\n",
    "        storage_offset,\n",
    "        size,\n",
    "        stride,\n",
    "        requires_grad,\n",
    "        backward_hooks,\n",
    "        metadata=None,\n",
    "        *,\n",
    "        archiveinfo=None,\n",
    "    ):\n",
    "        rebuild_args = (\n",
    "            storage_offset,\n",
    "            size,\n",
    "            stride,\n",
    "            requires_grad,\n",
    "            backward_hooks,\n",
    "            metadata,\n",
    "        )\n",
    "        metatensor = torch._utils._rebuild_tensor_v2(\n",
    "            storage,\n",
    "            storage_offset,\n",
    "            size,\n",
    "            stride,\n",
    "            requires_grad,\n",
    "            backward_hooks,\n",
    "            metadata,\n",
    "        )\n",
    "        storageinfo = storage.archiveinfo\n",
    "        return NotYetLoadedTensor(metatensor, archiveinfo, storageinfo, rebuild_args)\n",
    "\n",
    "    def _load_tensor(self):\n",
    "        name, storage_cls, fn, device, size = self.storageinfo\n",
    "        dtype = self.metatensor.dtype\n",
    "\n",
    "        uts = (\n",
    "            self.archiveinfo.zipfile_context.zf.get_storage_from_record(\n",
    "                f\"data/{fn}\",\n",
    "                size * torch._utils._element_size(dtype),\n",
    "                torch.UntypedStorage,\n",
    "            )\n",
    "            ._typed_storage()\n",
    "            ._untyped_storage\n",
    "        )\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            storage = torch.storage.TypedStorage(\n",
    "                wrap_storage=uts, dtype=self.metatensor.dtype, _internal=True\n",
    "            )\n",
    "        tensor = torch._utils._rebuild_tensor_v2(storage, *self.rebuild_args)\n",
    "        return tensor\n",
    "\n",
    "    @classmethod\n",
    "    def __torch_function__(cls, func, types, args=(), kwargs=None):\n",
    "        if kwargs is None:\n",
    "            kwargs = {}\n",
    "        loaded_args = [\n",
    "            (a._load_tensor() if isinstance(a, NotYetLoadedTensor) else a) for a in args\n",
    "        ]\n",
    "        res = func(*loaded_args, **kwargs)\n",
    "        # gc.collect would be costly here, maybe do it optionally\n",
    "        return res\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        # properties\n",
    "        ## TODO: device, is_...??\n",
    "        ## TODO: mH, mT, H, T, data, imag, real\n",
    "        ## name ???\n",
    "        if name in {\n",
    "            \"dtype\",\n",
    "            \"grad\",\n",
    "            \"grad_fn\",\n",
    "            \"layout\",\n",
    "            \"names\",\n",
    "            \"ndim\",\n",
    "            \"output_nr\",\n",
    "            \"requires_grad\",\n",
    "            \"retains_grad\",\n",
    "            \"shape\",\n",
    "            \"volatile\",\n",
    "        }:\n",
    "            return getattr(self.metatensor, name)\n",
    "        if name in {\"size\"}:\n",
    "            return getattr(self.metatensor, name)\n",
    "        # materializing with contiguous is needed for quantization\n",
    "        if name in {\"contiguous\"}:\n",
    "            return getattr(self._load_tensor(), name)\n",
    "\n",
    "        raise AttributeError(f\"{type(self)} does not have {name}\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"NotYetLoadedTensor({repr(self.metatensor)})\"\n",
    "    \n",
    "class LazyLoadingUnpickler(pickle.Unpickler):\n",
    "    def __init__(self, file, zipfile_context):\n",
    "        super().__init__(file)\n",
    "        self.zipfile_context = zipfile_context\n",
    "\n",
    "    def find_class(self, module, name):\n",
    "        res = super().find_class(module, name)\n",
    "        if module == \"torch._utils\" and name == \"_rebuild_tensor_v2\":\n",
    "            return functools.partial(\n",
    "                NotYetLoadedTensor.rebuild_tensor_v2, archiveinfo=self\n",
    "            )\n",
    "        elif module == \"torch._tensor\" and name == \"_rebuild_from_type_v2\":\n",
    "            return functools.partial(\n",
    "                NotYetLoadedTensor.rebuild_from_type_v2, archiveinfo=self\n",
    "            )\n",
    "        elif module == \"torch._utils\" and name == \"_rebuild_parameter\":\n",
    "            return functools.partial(\n",
    "                NotYetLoadedTensor.rebuild_parameter, archiveinfo=self\n",
    "            )\n",
    "        return res\n",
    "\n",
    "    def persistent_load(self, pid):\n",
    "        name, cls, fn, device, size = pid\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            s = torch.storage.TypedStorage(dtype=cls().dtype, device=\"meta\")\n",
    "        s.archiveinfo = pid\n",
    "        return s\n",
    "    \n",
    "class lazy_load:\n",
    "    def __init__(self, fn):\n",
    "        self.zf = torch._C.PyTorchFileReader(str(fn))\n",
    "        with BytesIO(self.zf.get_record(\"data.pkl\")) as pkl:\n",
    "            mup = LazyLoadingUnpickler(pkl, self)\n",
    "            self.sd = mup.load()\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self.sd\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        del self.zf  # I don't think there is a way to force closing...\n",
    "        self.zf = None\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model: LLaMA,\n",
    "    idx: torch.Tensor,\n",
    "    max_new_tokens: int,\n",
    "    *,\n",
    "    max_seq_length: Optional[int] = None,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: Optional[int] = None,\n",
    "    eos_id: Optional[int] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Takes a conditioning sequence (prompt) as input and continues to generate as many tokens as requested.\n",
    "\n",
    "    The implementation of this function is modified from A. Karpathy's nanoGPT.\n",
    "\n",
    "    Args:\n",
    "        model: The model to use.\n",
    "        idx: Tensor of shape (T) with indices of the prompt sequence.\n",
    "        max_new_tokens: The number of new tokens to generate.\n",
    "        max_seq_length: The maximum sequence length allowed.\n",
    "        temperature: Scales the predicted logits by 1 / temperature\n",
    "        top_k: If specified, only sample among the tokens with the k highest probabilities\n",
    "        eos_id: If specified, stop generating any more token once the <eos> token is triggered\n",
    "    \"\"\"\n",
    "    # create an empty tensor of the expected final shape and fill in the current tokens\n",
    "    T = idx.size(0)\n",
    "    T_new = T + max_new_tokens\n",
    "    if max_seq_length is None:\n",
    "        max_seq_length = min(T_new, model.config.block_size)\n",
    "\n",
    "    device, dtype = idx.device, idx.dtype\n",
    "    # create an empty tensor of the expected final shape and fill in the current tokens\n",
    "    empty = torch.empty(T_new, dtype=dtype, device=device)\n",
    "    empty[:T] = idx\n",
    "    idx = empty\n",
    "    input_pos = torch.arange(0, T, device=device)\n",
    "\n",
    "    # if idx.device.type == \"xla\":\n",
    "    #     import torch_xla.core.xla_model as xm\n",
    "\n",
    "    #     xm.mark_step()\n",
    "\n",
    "    # generate max_new_tokens tokens\n",
    "    for _ in range(max_new_tokens):\n",
    "        x = idx.index_select(0, input_pos).view(1, -1)\n",
    "\n",
    "        # forward\n",
    "        logits = model(x, max_seq_length, input_pos)\n",
    "        logits = logits[0, -1] / temperature\n",
    "\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits = torch.where(logits < v[[-1]], -float(\"Inf\"), logits)\n",
    "\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1).to(dtype=dtype)\n",
    "\n",
    "        # advance\n",
    "        input_pos = input_pos[-1:] + 1\n",
    "\n",
    "        # if idx.device.type == \"xla\":\n",
    "        #     xm.mark_step()\n",
    "\n",
    "        # concatenate the new generation\n",
    "        idx = idx.index_copy(0, input_pos, idx_next)\n",
    "\n",
    "        # if <eos> token is triggered, return the output (stop generation)\n",
    "        if idx_next == eos_id:\n",
    "            return idx[:input_pos]  # include the EOS token\n",
    "\n",
    "    return idx\n",
    "\n",
    "# The main function can be called directly in the notebook\n",
    "# Load pre-trained model and tokenizer\n",
    "checkpoint_path = \"out/training/iter-460000-ckpt.pth\"\n",
    "tokenizer_path = \"data/smiles/tokenizer.model\"\n",
    "\n",
    "assert Path(checkpoint_path).is_file(), checkpoint_path\n",
    "assert Path(tokenizer_path).is_file(), tokenizer_path\n",
    "\n",
    "print(\"Loading model ...\", file=sys.stderr)\n",
    "t0 = time.time()\n",
    "with lazy_load(checkpoint_path) as checkpoint:    \n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.to(device)\n",
    "print(f\"Time to load model: {time.time() - t0:.02f} seconds.\", file=sys.stderr)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC(=O)N[C@H]1CCC[C@@H]1CNC(=O)CNC(=O)C(C)(C)C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 1: 2.81 sec total, 13.54 tokens/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCCCNCC1(CCNC(=O)C(C)(C)CCOC)CC1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 2: 1.88 sec total, 13.84 tokens/sec\n",
      "Memory used: 2.06 GB\n"
     ]
    }
   ],
   "source": [
    "prompt = \"C\"\n",
    "num_samples = 2\n",
    "max_new_tokens = 100\n",
    "top_k = 200\n",
    "temperature = 1\n",
    "\n",
    "model.eval()\n",
    "tokenizer = Tokenizer(tokenizer_path)\n",
    "\n",
    "encoded = tokenizer.encode(prompt, bos=True, eos=False, device=device)\n",
    "prompt_length = encoded.size(0)\n",
    "\n",
    "# L.seed_everything(1234)\n",
    "for i in range(num_samples):\n",
    "    t0 = time.perf_counter()\n",
    "    y = generate(model, encoded, max_new_tokens, temperature=temperature, top_k=top_k, eos_id=tokenizer.eos_id)\n",
    "    t = time.perf_counter() - t0\n",
    "\n",
    "    model.reset_cache()\n",
    "    smiles_string = tokenizer.decode(y)\n",
    "    print(smiles_string)\n",
    "    tokens_generated = y.size(0) - prompt_length\n",
    "    print(f\"Time for inference {i + 1}: {t:.02f} sec total, {tokens_generated / t:.02f} tokens/sec\", file=sys.stderr)\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 50 molecules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 50: 2.32 sec total, 12.52 tokens/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 100 molecules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 100: 3.19 sec total, 11.60 tokens/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 150 molecules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 150: 2.91 sec total, 13.39 tokens/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 200 molecules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 200: 3.70 sec total, 11.09 tokens/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 250 molecules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 250: 2.42 sec total, 12.80 tokens/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 300 molecules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 300: 3.18 sec total, 13.84 tokens/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 350 molecules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 350: 3.49 sec total, 13.19 tokens/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 400 molecules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 400: 2.72 sec total, 10.28 tokens/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 450 molecules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 450: 3.02 sec total, 11.27 tokens/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 500 molecules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 500: 2.60 sec total, 12.30 tokens/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 550 molecules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 550: 2.36 sec total, 14.84 tokens/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 600 molecules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 600: 1.65 sec total, 16.93 tokens/sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 650 molecules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time for inference 650: 1.75 sec total, 18.34 tokens/sec\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Out of range: piece id is out of range.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects\\LLaMA-Mol\\3_mol_gen_pre-trained.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/3_mol_gen_pre-trained.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m t \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter() \u001b[39m-\u001b[39m t0\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/3_mol_gen_pre-trained.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m model\u001b[39m.\u001b[39mreset_cache()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/3_mol_gen_pre-trained.ipynb#X13sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m smiles_string \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/3_mol_gen_pre-trained.ipynb#X13sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m all_smiles\u001b[39m.\u001b[39mappend(smiles_string)   \u001b[39m# Append the generated molecule to the list\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/3_mol_gen_pre-trained.ipynb#X13sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Print progress after every 1000 molecules\u001b[39;00m\n",
      "\u001b[1;32md:\\Projects\\LLaMA-Mol\\3_mol_gen_pre-trained.ipynb Cell 11\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/3_mol_gen_pre-trained.ipynb#X13sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, tokens: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/3_mol_gen_pre-trained.ipynb#X13sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessor\u001b[39m.\u001b[39mdecode(tokens\u001b[39m.\u001b[39mtolist())\n",
      "File \u001b[1;32mc:\\Users\\m1000\\anaconda3\\envs\\my-rdkit-env\\Lib\\site-packages\\sentencepiece\\__init__.py:780\u001b[0m, in \u001b[0;36mSentencePieceProcessor.Decode\u001b[1;34m(self, input, out_type, num_threads)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39minput\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mlist\u001b[39m:\n\u001b[0;32m    779\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39minput\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39minput\u001b[39m[\u001b[39m0\u001b[39m]) \u001b[39mis\u001b[39;00m \u001b[39mint\u001b[39m:\n\u001b[1;32m--> 780\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_DecodeIds(\u001b[39minput\u001b[39m)\n\u001b[0;32m    781\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39minput\u001b[39m[\u001b[39m0\u001b[39m]) \u001b[39mis\u001b[39;00m \u001b[39mstr\u001b[39m:\n\u001b[0;32m    782\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_DecodePieces(\u001b[39minput\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\m1000\\anaconda3\\envs\\my-rdkit-env\\Lib\\site-packages\\sentencepiece\\__init__.py:337\u001b[0m, in \u001b[0;36mSentencePieceProcessor._DecodeIds\u001b[1;34m(self, ids)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_DecodeIds\u001b[39m(\u001b[39mself\u001b[39m, ids):\n\u001b[1;32m--> 337\u001b[0m     \u001b[39mreturn\u001b[39;00m _sentencepiece\u001b[39m.\u001b[39mSentencePieceProcessor__DecodeIds(\u001b[39mself\u001b[39m, ids)\n",
      "\u001b[1;31mIndexError\u001b[0m: Out of range: piece id is out of range."
     ]
    }
   ],
   "source": [
    "prompt = \"C\"\n",
    "num_samples = 50000   \n",
    "max_new_tokens = 100\n",
    "top_k = 200\n",
    "temperature = 1\n",
    "\n",
    "model.eval()\n",
    "tokenizer = Tokenizer(tokenizer_path)\n",
    "\n",
    "encoded = tokenizer.encode(prompt, bos=True, eos=False, device=device)\n",
    "prompt_length = encoded.size(0)\n",
    "\n",
    "all_smiles = []   # List to store the generated molecules\n",
    "\n",
    "# L.seed_everything(1234)\n",
    "\n",
    "# Instead of printing each molecule, store them in a list\n",
    "for i in range(num_samples):\n",
    "    t0 = time.perf_counter()\n",
    "    y = generate(model, encoded, max_new_tokens, temperature=temperature, top_k=top_k, eos_id=tokenizer.eos_id)\n",
    "    t = time.perf_counter() - t0\n",
    "\n",
    "    model.reset_cache()\n",
    "    smiles_string = tokenizer.decode(y)\n",
    "    all_smiles.append(smiles_string)   # Append the generated molecule to the list\n",
    "\n",
    "    # Print progress after every 1000 molecules\n",
    "    if (i + 1) % 50 == 0:\n",
    "        print(f\"Generated {i + 1} molecules\")\n",
    "        tokens_generated = y.size(0) - prompt_length\n",
    "        print(f\"Time for inference {i + 1}: {t:.02f} sec total, {tokens_generated / t:.02f} tokens/sec\", file=sys.stderr)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\", file=sys.stderr)\n",
    "\n",
    "# Optionally, save the generated molecules to a file\n",
    "with open(\"generated_molecules.txt\", \"w\") as f:\n",
    "    for smiles in all_smiles:\n",
    "        f.write(smiles + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"generated_molecules.txt\", \"w\") as f:\n",
    "    for smiles in all_smiles:\n",
    "        f.write(smiles + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-rdkit-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
