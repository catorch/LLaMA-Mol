{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightning in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: Jinja2<5.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (3.1.2)\n",
      "Requirement already satisfied: PyYAML<8.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (6.0)\n",
      "Requirement already satisfied: arrow<3.0,>=1.2.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (1.2.3)\n",
      "Requirement already satisfied: beautifulsoup4<6.0,>=4.8.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (4.12.2)\n",
      "Requirement already satisfied: click<10.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (8.1.3)\n",
      "Requirement already satisfied: croniter<1.4.0,>=1.3.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (1.3.15)\n",
      "Requirement already satisfied: dateutils<2.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (0.6.12)\n",
      "Requirement already satisfied: deepdiff<8.0,>=5.7.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (6.3.0)\n",
      "Requirement already satisfied: fastapi<0.89.0,>=0.69.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (0.88.0)\n",
      "Requirement already satisfied: fsspec<2024.0,>=2022.5.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (2023.5.0)\n",
      "Requirement already satisfied: inquirer<5.0,>=2.10.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (3.1.3)\n",
      "Requirement already satisfied: lightning-cloud>=0.5.34 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (0.5.36)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.7.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (0.8.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.17.2 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (1.24.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (23.0)\n",
      "Requirement already satisfied: psutil<7.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (5.9.0)\n",
      "Requirement already satisfied: pydantic<4.0,>=1.7.4 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (1.10.8)\n",
      "Requirement already satisfied: requests<4.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (2.28.1)\n",
      "Requirement already satisfied: rich<15.0,>=12.3.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (13.4.1)\n",
      "Requirement already satisfied: starlette in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (0.22.0)\n",
      "Requirement already satisfied: starsessions<2.0,>=1.2.1 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (1.3.0)\n",
      "Requirement already satisfied: torch<4.0,>=1.11.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (2.0.1+cu117)\n",
      "Requirement already satisfied: torchmetrics<2.0,>=0.7.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (0.11.4)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (4.65.0)\n",
      "Requirement already satisfied: traitlets<7.0,>=5.3.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (5.7.1)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (4.4.0)\n",
      "Requirement already satisfied: urllib3<3.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (1.26.13)\n",
      "Requirement already satisfied: uvicorn<2.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (0.22.0)\n",
      "Requirement already satisfied: websocket-client<3.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (1.5.2)\n",
      "Requirement already satisfied: websockets<12.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (11.0.3)\n",
      "Requirement already satisfied: pytorch-lightning in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from arrow<3.0,>=1.2.0->lightning) (2.8.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from beautifulsoup4<6.0,>=4.8.0->lightning) (2.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from click<10.0->lightning) (0.4.6)\n",
      "Requirement already satisfied: pytz in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from dateutils<2.0->lightning) (2022.7)\n",
      "Requirement already satisfied: ordered-set<4.2.0,>=4.0.2 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from deepdiff<8.0,>=5.7.0->lightning) (4.1.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from starlette->lightning) (3.7.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from fsspec<2024.0,>=2022.5.0->lightning) (3.8.4)\n",
      "Requirement already satisfied: blessed>=1.19.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from inquirer<5.0,>=2.10.0->lightning) (1.20.0)\n",
      "Requirement already satisfied: python-editor>=1.0.4 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from inquirer<5.0,>=2.10.0->lightning) (1.0.4)\n",
      "Requirement already satisfied: readchar>=3.0.6 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from inquirer<5.0,>=2.10.0->lightning) (4.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from Jinja2<5.0->lightning) (2.1.2)\n",
      "Requirement already satisfied: pyjwt in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning-cloud>=0.5.34->lightning) (2.7.0)\n",
      "Requirement already satisfied: python-multipart in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning-cloud>=0.5.34->lightning) (0.0.6)\n",
      "Requirement already satisfied: six in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from lightning-cloud>=0.5.34->lightning) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from requests<4.0->lightning) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from requests<4.0->lightning) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from requests<4.0->lightning) (2023.5.7)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from rich<15.0,>=12.3.0->lightning) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from rich<15.0,>=12.3.0->lightning) (2.15.1)\n",
      "Requirement already satisfied: itsdangerous<3.0.0,>=2.0.1 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from starsessions<2.0,>=1.2.1->lightning) (2.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from torch<4.0,>=1.11.0->lightning) (3.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from torch<4.0,>=1.11.0->lightning) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from torch<4.0,>=1.11.0->lightning) (3.0)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from uvicorn<2.0->lightning) (0.14.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning) (1.3.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from anyio<5,>=3.4.0->starlette->lightning) (1.3.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning) (0.2.5)\n",
      "Requirement already satisfied: jinxed>=1.1.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning) (1.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich<15.0,>=12.3.0->lightning) (0.1.2)\n",
      "Requirement already satisfied: setuptools>=41.0 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from readchar>=3.0.6->inquirer<5.0,>=2.10.0->lightning) (66.0.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from sympy->torch<4.0,>=1.11.0->lightning) (1.2.1)\n",
      "Requirement already satisfied: ansicon in c:\\users\\m1000\\anaconda3\\envs\\my-rdkit-env\\lib\\site-packages (from jinxed>=1.1.0->blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning) (1.89.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from typing_extensions import Self\n",
    "from lightning.fabric.strategies import DeepSpeedStrategy, FSDPStrategy\n",
    "from torch import distributed as dist\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaskCache = torch.Tensor\n",
    "RoPECache = torch.Tensor\n",
    "KVCache = Tuple[torch.Tensor, torch.Tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_multiple(n: int, k: int) -> int:\n",
    "    if n % k == 0:\n",
    "        return n\n",
    "    return n + k - (n % k)\n",
    "\n",
    "def save_model_checkpoint(model, file_path):\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    # Check if distributed training is initialized, if not, save the model directly\n",
    "    if not dist.is_initialized() or dist.get_rank() == 0:\n",
    "        state_dict = model.state_dict()\n",
    "        torch.save(state_dict, file_path)\n",
    "\n",
    "    # If distributed training is initialized, do a synchronization barrier after saving the model\n",
    "    if dist.is_initialized():\n",
    "        dist.barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_configs = {\n",
    "    \"7B\": dict(n_layer=32, n_head=32, n_embd=1024),\n",
    "    \"13B\": dict(n_layer=40, n_head=40, n_embd=5120),\n",
    "    \"30B\": dict(n_layer=60, n_head=52, n_embd=6656),\n",
    "    \"65B\": dict(n_layer=80, n_head=64, n_embd=8192),\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LLaMAConfig:\n",
    "    block_size: int = 2048\n",
    "    vocab_size: int = 32000\n",
    "    padded_vocab_size: Optional[int] = None\n",
    "    n_layer: int = 32\n",
    "    n_head: int = 32\n",
    "    n_embd: int = 4096\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.padded_vocab_size is None:\n",
    "            self.padded_vocab_size = find_multiple(self.vocab_size, 64)\n",
    "\n",
    "    @classmethod\n",
    "    def from_name(cls, name: str) -> Self:\n",
    "        return cls(**llama_configs[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLaMA(nn.Module):\n",
    "    def __init__(self, config: LLaMAConfig) -> None:\n",
    "        super().__init__()\n",
    "        assert config.padded_vocab_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.padded_vocab_size, bias=False)\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte=nn.Embedding(config.padded_vocab_size, config.n_embd),\n",
    "                h=nn.ModuleList(Block(config) for _ in range(config.n_layer)),\n",
    "                ln_f=RMSNorm(config.n_embd),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.rope_cache: Optional[RoPECache] = None\n",
    "        self.mask_cache: Optional[MaskCache] = None\n",
    "        self.kv_caches: List[KVCache] = []\n",
    "\n",
    "    def _init_weights(self, module: nn.Module) -> None:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layer))\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02 / math.sqrt(2 * self.config.n_layer))\n",
    "\n",
    "    def forward(\n",
    "        self, idx: torch.Tensor, max_seq_length: Optional[int] = None, input_pos: Optional[torch.Tensor] = None\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, List[KVCache]]]:\n",
    "        B, T = idx.size()\n",
    "\n",
    "        block_size = self.config.block_size\n",
    "        if max_seq_length is None:\n",
    "            max_seq_length = block_size\n",
    "        assert T <= max_seq_length, f\"Cannot forward sequence of length {T}, max seq length is only {max_seq_length}\"\n",
    "        assert max_seq_length <= block_size, f\"Cannot attend to {max_seq_length}, block size is only {block_size}\"\n",
    "        assert T <= block_size, f\"Cannot forward sequence of length {T}, block size is only {block_size}\"\n",
    "\n",
    "        if self.rope_cache is None:\n",
    "            self.rope_cache = self.build_rope_cache(idx)\n",
    "        if self.mask_cache is None:\n",
    "            self.mask_cache = self.build_mask_cache(idx)\n",
    "\n",
    "        if input_pos is not None:\n",
    "            rope = self.rope_cache.index_select(0, input_pos)\n",
    "            mask = self.mask_cache.index_select(2, input_pos)\n",
    "            mask = mask[:, :, :, :max_seq_length]\n",
    "        else:\n",
    "            rope = self.rope_cache[:T]\n",
    "            mask = self.mask_cache[:, :, :T, :T]\n",
    "\n",
    "        # forward the model itself\n",
    "        x = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n",
    "\n",
    "        if input_pos is None:  # proxy for use_cache=False\n",
    "            for block in self.transformer.h:\n",
    "                x, _ = block(x, rope, mask, max_seq_length)\n",
    "        else:\n",
    "            if not self.kv_caches:\n",
    "                head_size = self.config.n_embd // self.config.n_head\n",
    "                cache_shape = (B, self.config.n_head, max_seq_length, head_size)\n",
    "                self.kv_caches = [\n",
    "                    (torch.zeros(cache_shape, device=x.device, dtype=x.dtype), torch.zeros(cache_shape, device=x.device, dtype=x.dtype))\n",
    "                    for _ in range(self.config.n_layer)\n",
    "                ]\n",
    "            for i, block in enumerate(self.transformer.h):\n",
    "                x, self.kv_caches[i] = block(x, rope, mask, max_seq_length, input_pos, self.kv_caches[i])\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        logits = self.lm_head(x)  # (b, t, vocab_size)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    @classmethod\n",
    "    def from_name(cls, name: str) -> Self:\n",
    "        return cls(LLaMAConfig.from_name(name))\n",
    "\n",
    "    def build_rope_cache(self, idx: torch.Tensor) -> RoPECache:\n",
    "        return build_rope_cache(\n",
    "            seq_len=self.config.block_size,\n",
    "            n_elem=self.config.n_embd // self.config.n_head,\n",
    "            dtype=idx.dtype,\n",
    "            device=idx.device,\n",
    "        )\n",
    "\n",
    "    def build_mask_cache(self, idx: torch.Tensor) -> MaskCache:\n",
    "        ones = torch.ones((self.config.block_size, self.config.block_size), device=idx.device, dtype=torch.bool)\n",
    "        return torch.tril(ones).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    def reset_cache(self) -> None:\n",
    "        self.kv_caches.clear()\n",
    "        if self.mask_cache.device.type == \"xla\":\n",
    "            # https://github.com/Lightning-AI/lit-parrot/pull/83#issuecomment-1558150179\n",
    "            self.rope_cache = None\n",
    "            self.mask_cache = None\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config: LLaMAConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.rms_1 = RMSNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.rms_2 = RMSNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        rope: RoPECache,\n",
    "        mask: MaskCache,\n",
    "        max_seq_length: int,\n",
    "        input_pos: Optional[torch.Tensor] = None,\n",
    "        kv_cache: Optional[KVCache] = None,\n",
    "    ) -> Tuple[torch.Tensor, Optional[KVCache]]:\n",
    "        h, new_kv_cache = self.attn(self.rms_1(x), rope, mask, max_seq_length, input_pos, kv_cache)\n",
    "        x = x + h\n",
    "        x = x + self.mlp(self.rms_2(x))\n",
    "        return x, new_kv_cache\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config: LLaMAConfig) -> None:\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=False)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.block_size = config.block_size\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        rope: RoPECache,\n",
    "        mask: MaskCache,\n",
    "        max_seq_length: int,\n",
    "        input_pos: Optional[torch.Tensor] = None,\n",
    "        kv_cache: Optional[KVCache] = None,\n",
    "    ) -> Tuple[torch.Tensor, Optional[KVCache]]:\n",
    "        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "\n",
    "        head_size = C // self.n_head\n",
    "        k = k.view(B, T, self.n_head, head_size)\n",
    "        q = q.view(B, T, self.n_head, head_size)\n",
    "        v = v.view(B, T, self.n_head, head_size)\n",
    "\n",
    "        q = apply_rope(q, rope)\n",
    "        k = apply_rope(k, rope)\n",
    "\n",
    "        k = k.transpose(1, 2)  # (B, nh, T, hs)\n",
    "        q = q.transpose(1, 2)  # (B, nh, T, hs)\n",
    "        v = v.transpose(1, 2)  # (B, nh, T, hs)\n",
    "\n",
    "        if kv_cache is not None:\n",
    "            cache_k, cache_v = kv_cache\n",
    "            # check if reached token limit\n",
    "            if input_pos[-1] >= max_seq_length:\n",
    "                input_pos = torch.tensor(max_seq_length - 1, device=input_pos.device)\n",
    "                # shift 1 position to the left\n",
    "                cache_k = torch.roll(cache_k, -1, dims=2)\n",
    "                cache_v = torch.roll(cache_v, -1, dims=2)\n",
    "            k = cache_k.index_copy(2, input_pos, k)\n",
    "            v = cache_v.index_copy(2, input_pos, v)\n",
    "            kv_cache = k, v\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        #  att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        #  att = att.masked_fill(mask[:,:,:T,:T] == 0, float('-inf'))\n",
    "        #  att = F.softmax(att, dim=-1)\n",
    "        #  y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        # efficient attention using Flash Attention CUDA kernels\n",
    "        y = F.scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=0.0)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "\n",
    "        return y, kv_cache\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: LLaMAConfig) -> None:\n",
    "        super().__init__()\n",
    "        hidden_dim = 4 * config.n_embd\n",
    "        n_hidden = int(2 * hidden_dim / 3)\n",
    "        n_hidden = find_multiple(n_hidden, 256)\n",
    "\n",
    "        self.c_fc1 = nn.Linear(config.n_embd, n_hidden, bias=False)\n",
    "        self.c_fc2 = nn.Linear(config.n_embd, n_hidden, bias=False)\n",
    "        self.c_proj = nn.Linear(n_hidden, config.n_embd, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.silu(self.c_fc1(x)) * self.c_fc2(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization.\n",
    "\n",
    "    Derived from https://github.com/bzhangGo/rmsnorm/blob/master/rmsnorm_torch.py. BSD 3-Clause License:\n",
    "    https://github.com/bzhangGo/rmsnorm/blob/master/LICENSE.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size: int, dim: int = -1, eps: float = 1e-5) -> None:\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(size))\n",
    "        self.eps = eps\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # NOTE: the original RMSNorm paper implementation is not equivalent\n",
    "        # norm_x = x.norm(2, dim=self.dim, keepdim=True)\n",
    "        # rms_x = norm_x * d_x ** (-1. / 2)\n",
    "        # x_normed = x / (rms_x + self.eps)\n",
    "        norm_x = torch.mean(x * x, dim=self.dim, keepdim=True)\n",
    "        x_normed = x * torch.rsqrt(norm_x + self.eps)\n",
    "        return self.scale * x_normed\n",
    "\n",
    "\n",
    "def build_rope_cache(\n",
    "    seq_len: int, n_elem: int, dtype: torch.dtype, device: torch.device, base: int = 10000\n",
    ") -> RoPECache:\n",
    "    \"\"\"Enhanced Transformer with Rotary Position Embedding.\n",
    "\n",
    "    Derived from: https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/\n",
    "    transformers/rope/__init__.py. MIT License:\n",
    "    https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/license.\n",
    "    \"\"\"\n",
    "    # $\\Theta = {\\theta_i = 10000^{\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n",
    "    theta = 1.0 / (base ** (torch.arange(0, n_elem, 2, dtype=dtype, device=device) / n_elem))\n",
    "\n",
    "    # Create position indexes `[0, 1, ..., seq_len - 1]`\n",
    "    seq_idx = torch.arange(seq_len, dtype=dtype, device=device)\n",
    "\n",
    "    # Calculate the product of position index and $\\theta_i$\n",
    "    idx_theta = torch.outer(seq_idx, theta).float()\n",
    "\n",
    "    cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)\n",
    "\n",
    "    # this is to mimic the behaviour of complex32, else we will get different results\n",
    "    if dtype in (torch.float16, torch.bfloat16, torch.int8):\n",
    "        cache = cache.half()\n",
    "    return cache\n",
    "\n",
    "\n",
    "def apply_rope(x: torch.Tensor, rope_cache: RoPECache) -> torch.Tensor:\n",
    "    # truncate to support variable sizes\n",
    "    T = x.size(1)\n",
    "    rope_cache = rope_cache[:T]\n",
    "\n",
    "    # cast because the reference does\n",
    "    xshaped = x.float().reshape(*x.shape[:-1], -1, 2)\n",
    "    rope_cache = rope_cache.view(1, xshaped.size(1), 1, xshaped.size(3), 2)\n",
    "    x_out2 = torch.stack(\n",
    "        [\n",
    "            xshaped[..., 0] * rope_cache[..., 0] - xshaped[..., 1] * rope_cache[..., 1],\n",
    "            xshaped[..., 1] * rope_cache[..., 0] + xshaped[..., 0] * rope_cache[..., 1],\n",
    "        ],\n",
    "        -1,\n",
    "    )\n",
    "\n",
    "    x_out2 = x_out2.flatten(3)\n",
    "    return x_out2.type_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from sentencepiece import SentencePieceProcessor, SentencePieceTrainer\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"Tokenizer for LLaMA.\"\"\"\n",
    "\n",
    "    def __init__(self, model_path: Path) -> None:\n",
    "        self.processor = SentencePieceProcessor(model_file=str(model_path))\n",
    "        self.bos_id = self.processor.bos_id()\n",
    "        self.eos_id = self.processor.eos_id()\n",
    "        self.pad_id = self.processor.pad_id()\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return self.processor.vocab_size()\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        string: str,\n",
    "        bos: bool = True,\n",
    "        eos: bool = True,\n",
    "        max_length: int = -1,\n",
    "        pad: bool = False,\n",
    "        device: Optional[torch.device] = None\n",
    "    ) -> torch.Tensor:\n",
    "        lines = string.splitlines()\n",
    "        all_tokens = []\n",
    "        for line in lines:\n",
    "            tokens = self.processor.encode(line)\n",
    "            if bos:\n",
    "                tokens = [self.bos_id] + tokens\n",
    "            if eos:\n",
    "                tokens = tokens + [self.eos_id]\n",
    "            if max_length > 0:\n",
    "                tokens = tokens[:max_length]\n",
    "            if pad and len(tokens) < max_length:\n",
    "                tokens += [self.pad_id] * (max_length - len(tokens))\n",
    "            all_tokens.extend(tokens)\n",
    "        return torch.tensor(all_tokens, dtype=torch.int, device=device)\n",
    "\n",
    "    def decode(self, tokens: torch.Tensor) -> str:\n",
    "        return self.processor.decode(tokens.tolist())\n",
    "    \n",
    "    @staticmethod\n",
    "    def train(input: str, destination: str, vocab_size=32000) -> None:\n",
    "        model_prefix = os.path.join(destination, \"tokenizer\")\n",
    "        SentencePieceTrainer.Train(input=input, model_prefix=model_prefix, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>canonical_smiles</th>\n",
       "      <th>standard_value</th>\n",
       "      <th>type</th>\n",
       "      <th>units</th>\n",
       "      <th>ECFP4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O=C(O)/C=C/c1ccc(OS(=O)(=O)O)cc1</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>IC50</td>\n",
       "      <td>mM</td>\n",
       "      <td>['0', '0', '0', '0', '0', '0', '0', '0', '0', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CN(CCCNC(=O)c1ccc(O)cc1)CCCNC(=O)c1ccc(O)cc1</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>IC50</td>\n",
       "      <td>mM</td>\n",
       "      <td>['0', '0', '0', '0', '0', '0', '0', '0', '1', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCN(CCCN(CC)C(=O)c1ccc(O)cc1)C(=O)c1ccc(O)cc1</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>IC50</td>\n",
       "      <td>mM</td>\n",
       "      <td>['0', '0', '0', '0', '0', '0', '0', '0', '0', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CN(CCCNC(=O)c1ccc(O)cc1)CCCNC(=O)c1ccc2cc(O)cc...</td>\n",
       "      <td>294000.0</td>\n",
       "      <td>IC50</td>\n",
       "      <td>uM</td>\n",
       "      <td>['0', '0', '0', '0', '0', '0', '0', '0', '1', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCN(CCOC(=O)/C=C/c1ccc(O)cc1)Cc1cc(Cl)ccc1O</td>\n",
       "      <td>46000.0</td>\n",
       "      <td>IC50</td>\n",
       "      <td>uM</td>\n",
       "      <td>['0', '0', '0', '0', '0', '0', '0', '0', '0', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    canonical_smiles  standard_value  type  \\\n",
       "0                   O=C(O)/C=C/c1ccc(OS(=O)(=O)O)cc1       2000000.0  IC50   \n",
       "1       CN(CCCNC(=O)c1ccc(O)cc1)CCCNC(=O)c1ccc(O)cc1       2000000.0  IC50   \n",
       "2      CCN(CCCN(CC)C(=O)c1ccc(O)cc1)C(=O)c1ccc(O)cc1       2000000.0  IC50   \n",
       "3  CN(CCCNC(=O)c1ccc(O)cc1)CCCNC(=O)c1ccc2cc(O)cc...        294000.0  IC50   \n",
       "4        CCN(CCOC(=O)/C=C/c1ccc(O)cc1)Cc1cc(Cl)ccc1O         46000.0  IC50   \n",
       "\n",
       "  units                                              ECFP4  \n",
       "0    mM  ['0', '0', '0', '0', '0', '0', '0', '0', '0', ...  \n",
       "1    mM  ['0', '0', '0', '0', '0', '0', '0', '0', '1', ...  \n",
       "2    mM  ['0', '0', '0', '0', '0', '0', '0', '0', '0', ...  \n",
       "3    uM  ['0', '0', '0', '0', '0', '0', '0', '0', '1', ...  \n",
       "4    uM  ['0', '0', '0', '0', '0', '0', '0', '0', '0', ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"datasets/dengue_molecules.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['canonical_smiles'].to_csv(\"data/smiles/fine_tune_input.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\smiles\\fine_tune_input.txt\n",
      "<class 'str'>\n",
      "train has 72,449 tokens\n",
      "val has 7,805 tokens\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# support running without installing as a package\n",
    "# wd = Path(__file__).parent.parent.resolve()\n",
    "# sys.path.append(str(wd))\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "\n",
    "def prepare(destination_path: Path = Path(\"data/shakespeare\")) -> None:\n",
    "    \"\"\"Prepare the \"Tiny Shakespeare\" dataset.\"\"\"\n",
    "    destination_path.mkdir(parents=True, exist_ok=True)\n",
    "   \n",
    "    input_file_path = destination_path / \"fine_tune_input.txt\"\n",
    "    print(input_file_path)\n",
    "\n",
    "    with open(input_file_path) as f:\n",
    "        data = f.read()\n",
    "\n",
    "    # Add bos and eos tokens to each line\n",
    "    processed_lines = []\n",
    "\n",
    "    print(type(data))\n",
    "    n = len(data)\n",
    "    train_data = data[: int(n * 0.9)]\n",
    "    val_data = data[int(n * 0.9) :]\n",
    "\n",
    "    # Tokenizer.train(input=input_file_path, destination=destination_path, vocab_size=100)\n",
    "    tokenizer = Tokenizer(destination_path / \"tokenizer.model\")\n",
    "    train_ids = tokenizer.encode(train_data, device=device)\n",
    "    val_ids = tokenizer.encode(val_data, device=device)\n",
    "    print(f\"train has {len(train_ids):,} tokens\")\n",
    "    print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "    # Move tensors to CPU and convert to numpy arrays\n",
    "    train_ids = train_ids.cpu().numpy()\n",
    "    val_ids = val_ids.cpu().numpy()\n",
    "\n",
    "    # export to bin files\n",
    "    train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "    val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "    train_ids.tofile(destination_path / \"train.bin\")\n",
    "    val_ids.tofile(destination_path / \"val.bin\")\n",
    "\n",
    "\n",
    "prepare(Path(\"data/smiles/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from typing import Tuple\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_batch(data: np.ndarray, block_size: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i : i + block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i + 1 : i + 1 + block_size]).astype(np.int64)) for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "def load_datasets(data_dir: str = \"data/shakespeare\") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    train_data = np.memmap(os.path.join(data_dir, \"train.bin\"), dtype=np.uint16, mode=\"r\")\n",
    "    val_data = np.memmap(os.path.join(data_dir, \"val.bin\"), dtype=np.uint16, mode=\"r\")\n",
    "    return train_data, val_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    train_data: np.ndarray,\n",
    "    val_data: np.ndarray,\n",
    ") -> None:\n",
    "\n",
    "    iter_num = 0\n",
    "\n",
    "    while True:\n",
    "        # TODO: add learning rate scheduling\n",
    "\n",
    "        # evaluate the loss on train/val sets and write checkpoints\n",
    "        if iter_num > 0 and iter_num % eval_interval == 0:\n",
    "            val_loss = validate(model, val_data)\n",
    "            print(f\"step {iter_num}: val loss {val_loss:.4f}\")\n",
    "            print(f\"Saving checkpoint to {out_dir}\")\n",
    "            save_model_checkpoint(model, os.path.join(out_dir, f\"iter-{iter_num:06d}-ckpt.pth\"))\n",
    "            if val_loss <= 0.15:\n",
    "                break\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        input_ids, targets = get_batch(train_data, block_size=model.config.block_size)\n",
    "        input_ids, targets = input_ids.to(device), targets.to(device)\n",
    "        \n",
    "        logits = model(input_ids)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # TODO: Gradient clipping\n",
    "        # if grad_clip != 0.0:\n",
    "        #     torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        dt = time.time() - t0\n",
    "        if iter_num % log_interval == 0:\n",
    "            print(f\"iter {iter_num}: loss {loss.item():.4f}, time: {dt*1000:.2f}ms\")\n",
    "        iter_num += 1\n",
    "\n",
    "        if iter_num > max_iters:\n",
    "            break\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model: torch.nn.Module, val_data: np.ndarray) -> torch.Tensor:\n",
    "    print(\"Validating ...\")\n",
    "    model.eval()\n",
    "    losses = torch.zeros(eval_iters, device=device)\n",
    "    for k in range(eval_iters):\n",
    "        input_ids, targets = get_batch(val_data, block_size=model.config.block_size)\n",
    "        input_ids, targets = input_ids.to(device), targets.to(device)\n",
    "        \n",
    "        logits = model(input_ids)\n",
    "        loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        losses[k] = loss.item()\n",
    "    out = losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLaMA(\n",
       "  (lm_head): Linear(in_features=1024, out_features=32000, bias=False)\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(32000, 1024)\n",
       "    (h): ModuleList(\n",
       "      (0-31): 32 x Block(\n",
       "        (rms_1): RMSNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        )\n",
       "        (rms_2): RMSNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc1): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "          (c_fc2): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "          (c_proj): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): RMSNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "import os\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "def setup_ddp_module(module: torch.nn.Module) -> DDP:\n",
    "    return DDP(module, device_ids=[torch.cuda.current_device()])\n",
    "\n",
    "def setup_ddp_optimizers(optimizers: List[torch.optim.Optimizer]) -> List[torch.optim.Optimizer]:\n",
    "    return [torch.optim.AdamW(optimizer.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(beta1, beta2)) for optimizer in optimizers]\n",
    "\n",
    "\n",
    "\n",
    "out_dir = \"out/training\"\n",
    "eval_interval = 100\n",
    "eval_iters = 100\n",
    "log_interval = 1\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 6e-4\n",
    "batch_size = 2\n",
    "max_iters = 600000\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0\n",
    "\n",
    "# For shakespeare, choose smaller block size than vanilla LLaMA\n",
    "block_size = 1024\n",
    "\n",
    "# Seed setting\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# Directory creation\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Data loading\n",
    "train_data, val_data = load_datasets(\"data/smiles\")\n",
    "\n",
    "# Model configuration\n",
    "config = LLaMAConfig.from_name(\"7B\")\n",
    "config.block_size = block_size\n",
    "config.vocab_size = 100  # from prepare_shakespeare.py\n",
    "\n",
    "# Model initialization\n",
    "model = LLaMA(config)\n",
    "model.load_state_dict(torch.load(\"./out/training/iter-460000-ckpt.pth\"))\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Optimizer setting\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(beta1, beta2))\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: loss 1.4330, time: 11358.35ms\n",
      "iter 1: loss 1.1713, time: 797.92ms\n",
      "iter 2: loss 0.9882, time: 506.62ms\n",
      "iter 3: loss 1.0276, time: 507.65ms\n",
      "iter 4: loss 1.0086, time: 507.45ms\n",
      "iter 5: loss 0.9728, time: 508.11ms\n",
      "iter 6: loss 0.9494, time: 508.91ms\n",
      "iter 7: loss 0.9571, time: 510.20ms\n",
      "iter 8: loss 0.9789, time: 508.98ms\n",
      "iter 9: loss 0.9219, time: 511.21ms\n",
      "iter 10: loss 0.8833, time: 511.67ms\n",
      "iter 11: loss 0.8451, time: 511.92ms\n",
      "iter 12: loss 0.8521, time: 511.98ms\n",
      "iter 13: loss 0.9248, time: 510.77ms\n",
      "iter 14: loss 0.7688, time: 512.50ms\n",
      "iter 15: loss 0.8812, time: 511.25ms\n",
      "iter 16: loss 0.9770, time: 512.20ms\n",
      "iter 17: loss 0.8219, time: 514.01ms\n",
      "iter 18: loss 0.9264, time: 513.09ms\n",
      "iter 19: loss 0.7641, time: 514.18ms\n",
      "iter 20: loss 0.7612, time: 514.49ms\n",
      "iter 21: loss 0.7298, time: 522.06ms\n",
      "iter 22: loss 0.7315, time: 517.77ms\n",
      "iter 23: loss 0.8280, time: 514.75ms\n",
      "iter 24: loss 0.7219, time: 515.09ms\n",
      "iter 25: loss 0.8140, time: 515.26ms\n",
      "iter 26: loss 0.7014, time: 515.98ms\n",
      "iter 27: loss 0.7952, time: 515.72ms\n",
      "iter 28: loss 0.7063, time: 514.25ms\n",
      "iter 29: loss 0.6255, time: 515.09ms\n",
      "iter 30: loss 0.6832, time: 517.45ms\n",
      "iter 31: loss 0.5605, time: 516.95ms\n",
      "iter 32: loss 0.5603, time: 515.32ms\n",
      "iter 33: loss 0.6109, time: 516.39ms\n",
      "iter 34: loss 0.6998, time: 516.82ms\n",
      "iter 35: loss 0.6694, time: 516.37ms\n",
      "iter 36: loss 0.5770, time: 514.92ms\n",
      "iter 37: loss 0.6838, time: 518.38ms\n",
      "iter 38: loss 0.7018, time: 517.87ms\n",
      "iter 39: loss 0.5460, time: 519.81ms\n",
      "iter 40: loss 0.8326, time: 519.01ms\n",
      "iter 41: loss 0.6843, time: 519.56ms\n",
      "iter 42: loss 0.5368, time: 519.50ms\n",
      "iter 43: loss 0.4682, time: 519.03ms\n",
      "iter 44: loss 0.4981, time: 518.58ms\n",
      "iter 45: loss 0.6121, time: 520.23ms\n",
      "iter 46: loss 0.4824, time: 518.20ms\n",
      "iter 47: loss 0.6663, time: 518.92ms\n",
      "iter 48: loss 0.6031, time: 519.26ms\n",
      "iter 49: loss 0.6094, time: 520.43ms\n",
      "iter 50: loss 0.4421, time: 521.49ms\n",
      "iter 51: loss 0.5735, time: 519.81ms\n",
      "iter 52: loss 0.4080, time: 520.21ms\n",
      "iter 53: loss 0.5903, time: 519.61ms\n",
      "iter 54: loss 0.5100, time: 521.70ms\n",
      "iter 55: loss 0.5422, time: 521.75ms\n",
      "iter 56: loss 0.6046, time: 519.92ms\n",
      "iter 57: loss 0.5349, time: 522.85ms\n",
      "iter 58: loss 0.5173, time: 520.77ms\n",
      "iter 59: loss 0.5919, time: 524.18ms\n",
      "iter 60: loss 0.3844, time: 521.80ms\n",
      "iter 61: loss 0.5505, time: 522.62ms\n",
      "iter 62: loss 0.3881, time: 522.46ms\n",
      "iter 63: loss 0.3273, time: 521.48ms\n",
      "iter 64: loss 0.3999, time: 524.07ms\n",
      "iter 65: loss 0.5740, time: 524.10ms\n",
      "iter 66: loss 0.6425, time: 524.19ms\n",
      "iter 67: loss 0.5398, time: 523.05ms\n",
      "iter 68: loss 0.4632, time: 525.89ms\n",
      "iter 69: loss 0.3943, time: 521.94ms\n",
      "iter 70: loss 0.5525, time: 524.02ms\n",
      "iter 71: loss 0.4279, time: 522.62ms\n",
      "iter 72: loss 0.5048, time: 523.73ms\n",
      "iter 73: loss 0.4597, time: 524.54ms\n",
      "iter 74: loss 0.3813, time: 521.51ms\n",
      "iter 75: loss 0.3474, time: 525.81ms\n",
      "iter 76: loss 0.6465, time: 521.35ms\n",
      "iter 77: loss 0.4170, time: 524.82ms\n",
      "iter 78: loss 0.3942, time: 525.31ms\n",
      "iter 79: loss 0.4623, time: 525.80ms\n",
      "iter 80: loss 0.2101, time: 524.14ms\n",
      "iter 81: loss 0.3875, time: 525.15ms\n",
      "iter 82: loss 0.1852, time: 523.95ms\n",
      "iter 83: loss 0.3508, time: 526.18ms\n",
      "iter 84: loss 0.4758, time: 525.11ms\n",
      "iter 85: loss 0.4200, time: 524.75ms\n",
      "iter 86: loss 0.4420, time: 525.22ms\n",
      "iter 87: loss 0.4814, time: 524.60ms\n",
      "iter 88: loss 0.4320, time: 526.33ms\n",
      "iter 89: loss 0.4742, time: 526.84ms\n",
      "iter 90: loss 0.2239, time: 526.70ms\n",
      "iter 91: loss 0.3255, time: 526.29ms\n",
      "iter 92: loss 0.3321, time: 527.25ms\n",
      "iter 93: loss 0.3183, time: 525.89ms\n",
      "iter 94: loss 0.2249, time: 527.33ms\n",
      "iter 95: loss 0.4111, time: 527.19ms\n",
      "iter 96: loss 0.4531, time: 527.09ms\n",
      "iter 97: loss 0.1718, time: 528.61ms\n",
      "iter 98: loss 0.4214, time: 528.37ms\n",
      "iter 99: loss 0.3736, time: 527.57ms\n",
      "Validating ...\n",
      "step 100: val loss 0.9335\n",
      "Saving checkpoint to out/training\n",
      "iter 100: loss 0.3249, time: 774.35ms\n",
      "iter 101: loss 0.4189, time: 523.78ms\n",
      "iter 102: loss 0.2140, time: 522.80ms\n",
      "iter 103: loss 0.2284, time: 521.52ms\n",
      "iter 104: loss 0.4680, time: 522.19ms\n",
      "iter 105: loss 0.2269, time: 522.22ms\n",
      "iter 106: loss 0.1595, time: 523.46ms\n",
      "iter 107: loss 0.4252, time: 522.74ms\n",
      "iter 108: loss 0.4233, time: 522.01ms\n",
      "iter 109: loss 0.2734, time: 523.05ms\n",
      "iter 110: loss 0.3793, time: 522.87ms\n",
      "iter 111: loss 0.2531, time: 524.38ms\n",
      "iter 112: loss 0.4085, time: 523.62ms\n",
      "iter 113: loss 0.3541, time: 523.63ms\n",
      "iter 114: loss 0.5676, time: 525.12ms\n",
      "iter 115: loss 0.3733, time: 524.68ms\n",
      "iter 116: loss 0.3272, time: 524.76ms\n",
      "iter 117: loss 0.3856, time: 525.09ms\n",
      "iter 118: loss 0.3275, time: 526.37ms\n",
      "iter 119: loss 0.2762, time: 525.12ms\n",
      "iter 120: loss 0.4776, time: 524.65ms\n",
      "iter 121: loss 0.2655, time: 525.28ms\n",
      "iter 122: loss 0.3026, time: 528.01ms\n",
      "iter 123: loss 0.2600, time: 524.65ms\n",
      "iter 124: loss 0.3138, time: 525.60ms\n",
      "iter 125: loss 0.3119, time: 526.07ms\n",
      "iter 126: loss 0.2523, time: 526.97ms\n",
      "iter 127: loss 0.1480, time: 526.60ms\n",
      "iter 128: loss 0.4614, time: 525.12ms\n",
      "iter 129: loss 0.2053, time: 527.48ms\n",
      "iter 130: loss 0.3404, time: 527.24ms\n",
      "iter 131: loss 0.3531, time: 526.42ms\n",
      "iter 132: loss 0.3252, time: 526.94ms\n",
      "iter 133: loss 0.4490, time: 528.66ms\n",
      "iter 134: loss 0.3298, time: 531.06ms\n",
      "iter 135: loss 0.4260, time: 526.72ms\n",
      "iter 136: loss 0.4968, time: 529.54ms\n",
      "iter 137: loss 0.3583, time: 529.79ms\n",
      "iter 138: loss 0.3238, time: 532.39ms\n",
      "iter 139: loss 0.2659, time: 529.34ms\n",
      "iter 140: loss 0.3338, time: 530.42ms\n",
      "iter 141: loss 0.4061, time: 531.80ms\n",
      "iter 142: loss 0.3702, time: 531.33ms\n",
      "iter 143: loss 0.5262, time: 529.88ms\n",
      "iter 144: loss 0.2511, time: 532.18ms\n",
      "iter 145: loss 0.3787, time: 532.44ms\n",
      "iter 146: loss 0.3464, time: 533.43ms\n",
      "iter 147: loss 0.2834, time: 532.43ms\n",
      "iter 148: loss 0.3137, time: 531.73ms\n",
      "iter 149: loss 0.2601, time: 533.77ms\n",
      "iter 150: loss 0.2984, time: 532.43ms\n",
      "iter 151: loss 0.2174, time: 532.02ms\n",
      "iter 152: loss 0.2673, time: 533.99ms\n",
      "iter 153: loss 0.2630, time: 532.50ms\n",
      "iter 154: loss 0.3069, time: 534.24ms\n",
      "iter 155: loss 0.4519, time: 533.89ms\n",
      "iter 156: loss 0.1987, time: 535.83ms\n",
      "iter 157: loss 0.4108, time: 534.78ms\n",
      "iter 158: loss 0.3385, time: 534.51ms\n",
      "iter 159: loss 0.2431, time: 535.51ms\n",
      "iter 160: loss 0.2651, time: 533.25ms\n",
      "iter 161: loss 0.3251, time: 533.70ms\n",
      "iter 162: loss 0.1809, time: 531.60ms\n",
      "iter 163: loss 0.3267, time: 534.91ms\n",
      "iter 164: loss 0.3419, time: 533.44ms\n",
      "iter 165: loss 0.3687, time: 532.66ms\n",
      "iter 166: loss 0.3436, time: 533.92ms\n",
      "iter 167: loss 0.2878, time: 535.72ms\n",
      "iter 168: loss 0.2400, time: 531.13ms\n",
      "iter 169: loss 0.3159, time: 536.42ms\n",
      "iter 170: loss 0.1995, time: 534.51ms\n",
      "iter 171: loss 0.1777, time: 534.05ms\n",
      "iter 172: loss 0.2096, time: 536.87ms\n",
      "iter 173: loss 0.3940, time: 536.26ms\n",
      "iter 174: loss 0.2964, time: 534.27ms\n",
      "iter 175: loss 0.2148, time: 535.17ms\n",
      "iter 176: loss 0.3060, time: 537.00ms\n",
      "iter 177: loss 0.2928, time: 534.87ms\n",
      "iter 178: loss 0.2033, time: 535.07ms\n",
      "iter 179: loss 0.1722, time: 532.90ms\n",
      "iter 180: loss 0.1945, time: 537.57ms\n",
      "iter 181: loss 0.2540, time: 536.14ms\n",
      "iter 182: loss 0.3188, time: 533.99ms\n",
      "iter 183: loss 0.2508, time: 535.24ms\n",
      "iter 184: loss 0.1819, time: 537.19ms\n",
      "iter 185: loss 0.2966, time: 535.54ms\n",
      "iter 186: loss 0.2502, time: 535.36ms\n",
      "iter 187: loss 0.3031, time: 540.08ms\n",
      "iter 188: loss 0.2582, time: 535.34ms\n",
      "iter 189: loss 0.2393, time: 537.31ms\n",
      "iter 190: loss 0.2830, time: 537.89ms\n",
      "iter 191: loss 0.2694, time: 534.75ms\n",
      "iter 192: loss 0.2744, time: 535.40ms\n",
      "iter 193: loss 0.2090, time: 530.43ms\n",
      "iter 194: loss 0.1500, time: 537.05ms\n",
      "iter 195: loss 0.2657, time: 536.13ms\n",
      "iter 196: loss 0.2875, time: 531.02ms\n",
      "iter 197: loss 0.2667, time: 536.95ms\n",
      "iter 198: loss 0.3293, time: 538.94ms\n",
      "iter 199: loss 0.3098, time: 535.97ms\n",
      "Validating ...\n",
      "step 200: val loss 1.0829\n",
      "Saving checkpoint to out/training\n",
      "iter 200: loss 0.2486, time: 731.23ms\n",
      "iter 201: loss 0.2874, time: 522.68ms\n",
      "iter 202: loss 0.2565, time: 522.78ms\n",
      "iter 203: loss 0.3061, time: 525.98ms\n",
      "iter 204: loss 0.2876, time: 524.39ms\n",
      "iter 205: loss 0.2714, time: 523.49ms\n",
      "iter 206: loss 0.1984, time: 528.46ms\n",
      "iter 207: loss 0.2529, time: 524.13ms\n",
      "iter 208: loss 0.3111, time: 527.55ms\n",
      "iter 209: loss 0.2038, time: 526.47ms\n",
      "iter 210: loss 0.2671, time: 526.77ms\n",
      "iter 211: loss 0.3528, time: 524.74ms\n",
      "iter 212: loss 0.3580, time: 525.43ms\n",
      "iter 213: loss 0.3731, time: 525.57ms\n",
      "iter 214: loss 0.2111, time: 526.71ms\n",
      "iter 215: loss 0.2573, time: 525.27ms\n",
      "iter 216: loss 0.2849, time: 525.99ms\n",
      "iter 217: loss 0.2877, time: 525.89ms\n",
      "iter 218: loss 0.3537, time: 525.51ms\n",
      "iter 219: loss 0.1994, time: 525.17ms\n",
      "iter 220: loss 0.2236, time: 527.97ms\n",
      "iter 221: loss 0.1823, time: 527.22ms\n",
      "iter 222: loss 0.2127, time: 528.20ms\n",
      "iter 223: loss 0.2560, time: 528.00ms\n",
      "iter 224: loss 0.3522, time: 530.23ms\n",
      "iter 225: loss 0.1884, time: 530.34ms\n",
      "iter 226: loss 0.2592, time: 530.05ms\n",
      "iter 227: loss 0.1584, time: 532.53ms\n",
      "iter 228: loss 0.2620, time: 530.44ms\n",
      "iter 229: loss 0.1766, time: 530.24ms\n",
      "iter 230: loss 0.2704, time: 530.98ms\n",
      "iter 231: loss 0.1952, time: 531.54ms\n",
      "iter 232: loss 0.1276, time: 531.20ms\n",
      "iter 233: loss 0.2067, time: 532.90ms\n",
      "iter 234: loss 0.1651, time: 532.13ms\n",
      "iter 235: loss 0.2319, time: 534.22ms\n",
      "iter 236: loss 0.1986, time: 532.32ms\n",
      "iter 237: loss 0.2840, time: 534.02ms\n",
      "iter 238: loss 0.2159, time: 536.12ms\n",
      "iter 239: loss 0.2378, time: 534.72ms\n",
      "iter 240: loss 0.2112, time: 533.18ms\n",
      "iter 241: loss 0.1168, time: 536.47ms\n",
      "iter 242: loss 0.1686, time: 535.46ms\n",
      "iter 243: loss 0.2035, time: 533.94ms\n",
      "iter 244: loss 0.2079, time: 537.17ms\n",
      "iter 245: loss 0.1902, time: 536.72ms\n",
      "iter 246: loss 0.2189, time: 536.66ms\n",
      "iter 247: loss 0.2414, time: 532.06ms\n",
      "iter 248: loss 0.2189, time: 537.79ms\n",
      "iter 249: loss 0.2086, time: 537.49ms\n",
      "iter 250: loss 0.2055, time: 536.65ms\n",
      "iter 251: loss 0.2237, time: 538.25ms\n",
      "iter 252: loss 0.3260, time: 537.83ms\n",
      "iter 253: loss 0.1879, time: 535.56ms\n",
      "iter 254: loss 0.0927, time: 537.05ms\n",
      "iter 255: loss 0.2045, time: 537.95ms\n",
      "iter 256: loss 0.2391, time: 536.86ms\n",
      "iter 257: loss 0.2713, time: 538.00ms\n",
      "iter 258: loss 0.1563, time: 535.53ms\n",
      "iter 259: loss 0.3602, time: 539.82ms\n",
      "iter 260: loss 0.1916, time: 538.71ms\n",
      "iter 261: loss 0.1691, time: 537.26ms\n",
      "iter 262: loss 0.3112, time: 539.09ms\n",
      "iter 263: loss 0.2063, time: 539.54ms\n",
      "iter 264: loss 0.1379, time: 538.55ms\n",
      "iter 265: loss 0.2579, time: 540.79ms\n",
      "iter 266: loss 0.2294, time: 530.20ms\n",
      "iter 267: loss 0.1966, time: 541.11ms\n",
      "iter 268: loss 0.2158, time: 540.09ms\n",
      "iter 269: loss 0.2418, time: 539.37ms\n",
      "iter 270: loss 0.1487, time: 539.69ms\n",
      "iter 271: loss 0.1195, time: 539.91ms\n",
      "iter 272: loss 0.2307, time: 538.28ms\n",
      "iter 273: loss 0.1545, time: 542.52ms\n",
      "iter 274: loss 0.2189, time: 534.42ms\n",
      "iter 275: loss 0.2243, time: 542.01ms\n",
      "iter 276: loss 0.1914, time: 542.05ms\n",
      "iter 277: loss 0.2748, time: 538.72ms\n",
      "iter 278: loss 0.1242, time: 540.33ms\n",
      "iter 279: loss 0.2141, time: 540.67ms\n",
      "iter 280: loss 0.2051, time: 539.62ms\n",
      "iter 281: loss 0.2770, time: 540.11ms\n",
      "iter 282: loss 0.1569, time: 536.73ms\n",
      "iter 283: loss 0.2071, time: 540.75ms\n",
      "iter 284: loss 0.1540, time: 542.86ms\n",
      "iter 285: loss 0.2040, time: 539.37ms\n",
      "iter 286: loss 0.1594, time: 543.76ms\n",
      "iter 287: loss 0.1268, time: 532.91ms\n",
      "iter 288: loss 0.2386, time: 543.94ms\n",
      "iter 289: loss 0.0982, time: 543.56ms\n",
      "iter 290: loss 0.1207, time: 539.03ms\n",
      "iter 291: loss 0.2284, time: 541.90ms\n",
      "iter 292: loss 0.2282, time: 542.22ms\n",
      "iter 293: loss 0.2158, time: 543.22ms\n",
      "iter 294: loss 0.1804, time: 541.63ms\n",
      "iter 295: loss 0.1551, time: 534.81ms\n",
      "iter 296: loss 0.1820, time: 546.32ms\n",
      "iter 297: loss 0.1397, time: 540.83ms\n",
      "iter 298: loss 0.1835, time: 540.29ms\n",
      "iter 299: loss 0.1784, time: 544.53ms\n",
      "Validating ...\n",
      "step 300: val loss 1.1649\n",
      "Saving checkpoint to out/training\n",
      "iter 300: loss 0.2442, time: 824.05ms\n",
      "iter 301: loss 0.1058, time: 523.06ms\n",
      "iter 302: loss 0.1092, time: 524.15ms\n",
      "iter 303: loss 0.2831, time: 523.88ms\n",
      "iter 304: loss 0.1190, time: 524.91ms\n",
      "iter 305: loss 0.2041, time: 524.73ms\n",
      "iter 306: loss 0.1533, time: 524.49ms\n",
      "iter 307: loss 0.1508, time: 524.56ms\n",
      "iter 308: loss 0.1092, time: 525.07ms\n",
      "iter 309: loss 0.1979, time: 526.79ms\n",
      "iter 310: loss 0.2130, time: 525.33ms\n",
      "iter 311: loss 0.1789, time: 526.55ms\n",
      "iter 312: loss 0.1012, time: 525.87ms\n",
      "iter 313: loss 0.1282, time: 526.19ms\n",
      "iter 314: loss 0.1108, time: 526.93ms\n",
      "iter 315: loss 0.2569, time: 525.65ms\n",
      "iter 316: loss 0.1879, time: 527.61ms\n",
      "iter 317: loss 0.2866, time: 527.63ms\n",
      "iter 318: loss 0.1339, time: 528.17ms\n",
      "iter 319: loss 0.1098, time: 530.17ms\n",
      "iter 320: loss 0.2066, time: 529.73ms\n",
      "iter 321: loss 0.2197, time: 531.30ms\n",
      "iter 322: loss 0.1783, time: 531.65ms\n",
      "iter 323: loss 0.1436, time: 533.25ms\n",
      "iter 324: loss 0.1978, time: 531.26ms\n",
      "iter 325: loss 0.1848, time: 531.44ms\n",
      "iter 326: loss 0.1454, time: 533.07ms\n",
      "iter 327: loss 0.1745, time: 531.54ms\n",
      "iter 328: loss 0.1396, time: 533.82ms\n",
      "iter 329: loss 0.1200, time: 533.03ms\n",
      "iter 330: loss 0.2154, time: 534.54ms\n",
      "iter 331: loss 0.1713, time: 537.94ms\n",
      "iter 332: loss 0.1672, time: 535.15ms\n",
      "iter 333: loss 0.2182, time: 537.68ms\n",
      "iter 334: loss 0.1717, time: 534.14ms\n",
      "iter 335: loss 0.1371, time: 535.89ms\n",
      "iter 336: loss 0.1362, time: 538.00ms\n",
      "iter 337: loss 0.2310, time: 533.74ms\n",
      "iter 338: loss 0.1385, time: 536.77ms\n",
      "iter 339: loss 0.2276, time: 537.54ms\n",
      "iter 340: loss 0.1179, time: 536.89ms\n",
      "iter 341: loss 0.1168, time: 537.81ms\n",
      "iter 342: loss 0.1810, time: 537.31ms\n",
      "iter 343: loss 0.1375, time: 539.27ms\n",
      "iter 344: loss 0.1443, time: 538.99ms\n",
      "iter 345: loss 0.1163, time: 532.33ms\n",
      "iter 346: loss 0.1098, time: 538.80ms\n",
      "iter 347: loss 0.2070, time: 542.01ms\n",
      "iter 348: loss 0.1357, time: 538.05ms\n",
      "iter 349: loss 0.1862, time: 538.56ms\n",
      "iter 350: loss 0.1715, time: 542.21ms\n",
      "iter 351: loss 0.1203, time: 538.95ms\n",
      "iter 352: loss 0.1336, time: 540.11ms\n",
      "iter 353: loss 0.1881, time: 531.90ms\n",
      "iter 354: loss 0.2029, time: 543.55ms\n",
      "iter 355: loss 0.0983, time: 540.96ms\n",
      "iter 356: loss 0.1488, time: 537.24ms\n",
      "iter 357: loss 0.1367, time: 540.86ms\n",
      "iter 358: loss 0.1055, time: 541.05ms\n",
      "iter 359: loss 0.1717, time: 540.86ms\n",
      "iter 360: loss 0.2577, time: 542.79ms\n",
      "iter 361: loss 0.1539, time: 534.27ms\n",
      "iter 362: loss 0.1792, time: 542.26ms\n",
      "iter 363: loss 0.2002, time: 542.44ms\n",
      "iter 364: loss 0.1840, time: 542.47ms\n",
      "iter 365: loss 0.1495, time: 542.06ms\n",
      "iter 366: loss 0.1250, time: 535.99ms\n",
      "iter 367: loss 0.1365, time: 542.91ms\n",
      "iter 368: loss 0.1473, time: 543.24ms\n",
      "iter 369: loss 0.0862, time: 540.60ms\n",
      "iter 370: loss 0.1776, time: 544.92ms\n",
      "iter 371: loss 0.1940, time: 539.88ms\n",
      "iter 372: loss 0.1204, time: 542.92ms\n",
      "iter 373: loss 0.1263, time: 542.00ms\n",
      "iter 374: loss 0.1469, time: 540.96ms\n",
      "iter 375: loss 0.1064, time: 544.39ms\n",
      "iter 376: loss 0.0903, time: 540.34ms\n",
      "iter 377: loss 0.1005, time: 547.86ms\n",
      "iter 378: loss 0.1256, time: 544.79ms\n",
      "iter 379: loss 0.1742, time: 541.04ms\n",
      "iter 380: loss 0.1873, time: 551.99ms\n",
      "iter 381: loss 0.0938, time: 536.09ms\n",
      "iter 382: loss 0.1419, time: 547.10ms\n",
      "iter 383: loss 0.1689, time: 545.17ms\n",
      "iter 384: loss 0.0714, time: 542.42ms\n",
      "iter 385: loss 0.1594, time: 544.37ms\n",
      "iter 386: loss 0.1167, time: 542.50ms\n",
      "iter 387: loss 0.1108, time: 545.36ms\n",
      "iter 388: loss 0.1250, time: 547.18ms\n",
      "iter 389: loss 0.1142, time: 545.60ms\n",
      "iter 390: loss 0.1542, time: 543.47ms\n",
      "iter 391: loss 0.1113, time: 537.12ms\n",
      "iter 392: loss 0.1822, time: 544.35ms\n",
      "iter 393: loss 0.1532, time: 545.35ms\n",
      "iter 394: loss 0.1390, time: 540.74ms\n",
      "iter 395: loss 0.0929, time: 545.55ms\n",
      "iter 396: loss 0.0983, time: 532.00ms\n",
      "iter 397: loss 0.1088, time: 548.75ms\n",
      "iter 398: loss 0.1457, time: 547.71ms\n",
      "iter 399: loss 0.1492, time: 541.12ms\n",
      "Validating ...\n",
      "step 400: val loss 1.3406\n",
      "Saving checkpoint to out/training\n",
      "iter 400: loss 0.0994, time: 818.45ms\n",
      "iter 401: loss 0.0764, time: 524.77ms\n",
      "iter 402: loss 0.0895, time: 524.88ms\n",
      "iter 403: loss 0.1412, time: 525.87ms\n",
      "iter 404: loss 0.2021, time: 523.94ms\n",
      "iter 405: loss 0.1244, time: 523.47ms\n",
      "iter 406: loss 0.1279, time: 526.21ms\n",
      "iter 407: loss 0.1395, time: 527.75ms\n",
      "iter 408: loss 0.1146, time: 524.87ms\n",
      "iter 409: loss 0.1700, time: 528.17ms\n",
      "iter 410: loss 0.1483, time: 526.63ms\n",
      "iter 411: loss 0.0913, time: 526.20ms\n",
      "iter 412: loss 0.0976, time: 527.60ms\n",
      "iter 413: loss 0.1324, time: 529.93ms\n",
      "iter 414: loss 0.0967, time: 528.70ms\n",
      "iter 415: loss 0.1710, time: 530.00ms\n",
      "iter 416: loss 0.1378, time: 532.17ms\n",
      "iter 417: loss 0.1124, time: 531.54ms\n",
      "iter 418: loss 0.1235, time: 529.91ms\n",
      "iter 419: loss 0.0936, time: 529.88ms\n",
      "iter 420: loss 0.0810, time: 531.26ms\n",
      "iter 421: loss 0.0997, time: 532.84ms\n",
      "iter 422: loss 0.0840, time: 530.78ms\n",
      "iter 423: loss 0.1483, time: 533.26ms\n",
      "iter 424: loss 0.1957, time: 534.13ms\n",
      "iter 425: loss 0.1218, time: 533.15ms\n",
      "iter 426: loss 0.1005, time: 531.99ms\n",
      "iter 427: loss 0.0914, time: 535.47ms\n",
      "iter 428: loss 0.0994, time: 534.31ms\n",
      "iter 429: loss 0.0960, time: 533.41ms\n",
      "iter 430: loss 0.1421, time: 538.38ms\n",
      "iter 431: loss 0.0909, time: 535.00ms\n",
      "iter 432: loss 0.1284, time: 534.22ms\n",
      "iter 433: loss 0.1092, time: 529.75ms\n",
      "iter 434: loss 0.1150, time: 538.27ms\n",
      "iter 435: loss 0.0830, time: 537.39ms\n",
      "iter 436: loss 0.1505, time: 533.07ms\n",
      "iter 437: loss 0.1058, time: 538.35ms\n",
      "iter 438: loss 0.1182, time: 541.20ms\n",
      "iter 439: loss 0.1284, time: 537.83ms\n",
      "iter 440: loss 0.0927, time: 540.04ms\n",
      "iter 441: loss 0.0673, time: 538.85ms\n",
      "iter 442: loss 0.1131, time: 539.18ms\n",
      "iter 443: loss 0.1050, time: 540.71ms\n",
      "iter 444: loss 0.1450, time: 538.45ms\n",
      "iter 445: loss 0.0938, time: 540.93ms\n",
      "iter 446: loss 0.1019, time: 542.88ms\n",
      "iter 447: loss 0.1024, time: 541.31ms\n",
      "iter 448: loss 0.1100, time: 542.47ms\n",
      "iter 449: loss 0.0859, time: 535.05ms\n",
      "iter 450: loss 0.1461, time: 540.92ms\n",
      "iter 451: loss 0.1020, time: 544.00ms\n",
      "iter 452: loss 0.1391, time: 540.04ms\n",
      "iter 453: loss 0.1013, time: 541.60ms\n",
      "iter 454: loss 0.0703, time: 543.17ms\n",
      "iter 455: loss 0.1001, time: 544.27ms\n",
      "iter 456: loss 0.1154, time: 544.46ms\n",
      "iter 457: loss 0.1135, time: 538.98ms\n",
      "iter 458: loss 0.1054, time: 545.16ms\n",
      "iter 459: loss 0.0876, time: 545.70ms\n",
      "iter 460: loss 0.0957, time: 543.79ms\n",
      "iter 461: loss 0.0756, time: 544.33ms\n",
      "iter 462: loss 0.0929, time: 540.82ms\n",
      "iter 463: loss 0.1314, time: 544.57ms\n",
      "iter 464: loss 0.1053, time: 539.78ms\n",
      "iter 465: loss 0.0668, time: 542.77ms\n",
      "iter 466: loss 0.0869, time: 543.43ms\n",
      "iter 467: loss 0.1039, time: 539.03ms\n",
      "iter 468: loss 0.1149, time: 548.22ms\n",
      "iter 469: loss 0.1054, time: 541.11ms\n",
      "iter 470: loss 0.1030, time: 544.20ms\n",
      "iter 471: loss 0.1480, time: 545.22ms\n",
      "iter 472: loss 0.0674, time: 543.71ms\n",
      "iter 473: loss 0.1277, time: 544.40ms\n",
      "iter 474: loss 0.0939, time: 531.92ms\n",
      "iter 475: loss 0.1103, time: 547.41ms\n",
      "iter 476: loss 0.0963, time: 546.78ms\n",
      "iter 477: loss 0.0964, time: 544.00ms\n",
      "iter 478: loss 0.0774, time: 547.31ms\n",
      "iter 479: loss 0.0947, time: 539.79ms\n",
      "iter 480: loss 0.0808, time: 546.89ms\n",
      "iter 481: loss 0.0812, time: 546.62ms\n",
      "iter 482: loss 0.0854, time: 544.99ms\n",
      "iter 483: loss 0.1125, time: 543.33ms\n",
      "iter 484: loss 0.0912, time: 534.07ms\n",
      "iter 485: loss 0.0693, time: 547.25ms\n",
      "iter 486: loss 0.1086, time: 547.71ms\n",
      "iter 487: loss 0.0787, time: 545.75ms\n",
      "iter 488: loss 0.0634, time: 544.87ms\n",
      "iter 489: loss 0.0857, time: 545.18ms\n",
      "iter 490: loss 0.0722, time: 545.21ms\n",
      "iter 491: loss 0.0740, time: 539.21ms\n",
      "iter 492: loss 0.1168, time: 547.29ms\n",
      "iter 493: loss 0.0907, time: 545.21ms\n",
      "iter 494: loss 0.1014, time: 545.48ms\n",
      "iter 495: loss 0.1443, time: 550.80ms\n",
      "iter 496: loss 0.1792, time: 540.09ms\n",
      "iter 497: loss 0.0693, time: 545.13ms\n",
      "iter 498: loss 0.0837, time: 547.00ms\n",
      "iter 499: loss 0.0917, time: 546.90ms\n",
      "Validating ...\n",
      "step 500: val loss 1.2991\n",
      "Saving checkpoint to out/training\n",
      "iter 500: loss 0.0792, time: 764.98ms\n",
      "iter 501: loss 0.0675, time: 525.08ms\n",
      "iter 502: loss 0.0958, time: 522.84ms\n",
      "iter 503: loss 0.0702, time: 524.47ms\n",
      "iter 504: loss 0.0957, time: 523.60ms\n",
      "iter 505: loss 0.0822, time: 525.33ms\n",
      "iter 506: loss 0.0655, time: 524.64ms\n",
      "iter 507: loss 0.0694, time: 525.24ms\n",
      "iter 508: loss 0.0798, time: 525.01ms\n",
      "iter 509: loss 0.0834, time: 527.34ms\n",
      "iter 510: loss 0.1319, time: 526.85ms\n",
      "iter 511: loss 0.0722, time: 528.13ms\n",
      "iter 512: loss 0.1195, time: 528.74ms\n",
      "iter 513: loss 0.0951, time: 529.24ms\n",
      "iter 514: loss 0.0870, time: 528.52ms\n",
      "iter 515: loss 0.0758, time: 528.96ms\n",
      "iter 516: loss 0.0668, time: 529.27ms\n",
      "iter 517: loss 0.0831, time: 531.39ms\n",
      "iter 518: loss 0.1179, time: 532.29ms\n",
      "iter 519: loss 0.1172, time: 528.34ms\n",
      "iter 520: loss 0.1115, time: 533.09ms\n",
      "iter 521: loss 0.0813, time: 533.85ms\n",
      "iter 522: loss 0.0967, time: 532.13ms\n",
      "iter 523: loss 0.0798, time: 533.36ms\n",
      "iter 524: loss 0.0677, time: 536.67ms\n",
      "iter 525: loss 0.0620, time: 531.92ms\n",
      "iter 526: loss 0.0751, time: 535.26ms\n",
      "iter 527: loss 0.1313, time: 535.16ms\n",
      "iter 528: loss 0.0716, time: 536.42ms\n",
      "iter 529: loss 0.0841, time: 538.62ms\n",
      "iter 530: loss 0.0752, time: 537.09ms\n",
      "iter 531: loss 0.1264, time: 538.15ms\n",
      "iter 532: loss 0.0544, time: 537.78ms\n",
      "iter 533: loss 0.0762, time: 529.79ms\n",
      "iter 534: loss 0.0666, time: 542.99ms\n",
      "iter 535: loss 0.0748, time: 540.41ms\n",
      "iter 536: loss 0.0834, time: 533.07ms\n",
      "iter 537: loss 0.0953, time: 542.55ms\n",
      "iter 538: loss 0.1080, time: 545.11ms\n",
      "iter 539: loss 0.0818, time: 542.59ms\n",
      "iter 540: loss 0.0661, time: 539.90ms\n",
      "iter 541: loss 0.0885, time: 534.48ms\n",
      "iter 542: loss 0.0828, time: 543.67ms\n",
      "iter 543: loss 0.0642, time: 542.96ms\n",
      "iter 544: loss 0.0990, time: 539.19ms\n",
      "iter 545: loss 0.1095, time: 543.43ms\n",
      "iter 546: loss 0.0673, time: 541.24ms\n",
      "iter 547: loss 0.0695, time: 542.98ms\n",
      "iter 548: loss 0.0693, time: 542.92ms\n",
      "iter 549: loss 0.0587, time: 540.13ms\n",
      "iter 550: loss 0.0752, time: 544.67ms\n",
      "iter 551: loss 0.0698, time: 542.33ms\n",
      "iter 552: loss 0.1162, time: 544.47ms\n",
      "iter 553: loss 0.0877, time: 545.60ms\n",
      "iter 554: loss 0.0702, time: 540.71ms\n",
      "iter 555: loss 0.1839, time: 544.58ms\n",
      "iter 556: loss 0.0812, time: 540.41ms\n",
      "iter 557: loss 0.0737, time: 544.92ms\n",
      "iter 558: loss 0.1144, time: 546.59ms\n",
      "iter 559: loss 0.0827, time: 542.29ms\n",
      "iter 560: loss 0.0881, time: 543.28ms\n",
      "iter 561: loss 0.0644, time: 541.51ms\n",
      "iter 562: loss 0.0764, time: 545.36ms\n",
      "iter 563: loss 0.0462, time: 547.62ms\n",
      "iter 564: loss 0.1044, time: 542.79ms\n",
      "iter 565: loss 0.0880, time: 546.18ms\n",
      "iter 566: loss 0.1014, time: 533.64ms\n",
      "iter 567: loss 0.0715, time: 547.63ms\n",
      "iter 568: loss 0.0738, time: 547.06ms\n",
      "iter 569: loss 0.0852, time: 545.68ms\n",
      "iter 570: loss 0.0897, time: 545.34ms\n",
      "iter 571: loss 0.0916, time: 540.98ms\n",
      "iter 572: loss 0.0768, time: 547.01ms\n",
      "iter 573: loss 0.0746, time: 549.81ms\n",
      "iter 574: loss 0.0868, time: 546.45ms\n",
      "iter 575: loss 0.0652, time: 546.73ms\n",
      "iter 576: loss 0.0825, time: 546.05ms\n",
      "iter 577: loss 0.1097, time: 545.15ms\n",
      "iter 578: loss 0.0828, time: 539.09ms\n",
      "iter 579: loss 0.0584, time: 547.99ms\n",
      "iter 580: loss 0.0656, time: 547.46ms\n",
      "iter 581: loss 0.0712, time: 545.11ms\n",
      "iter 582: loss 0.1026, time: 546.96ms\n",
      "iter 583: loss 0.0801, time: 543.74ms\n",
      "iter 584: loss 0.0596, time: 545.55ms\n",
      "iter 585: loss 0.0664, time: 542.64ms\n",
      "iter 586: loss 0.0758, time: 546.61ms\n",
      "iter 587: loss 0.0737, time: 548.49ms\n",
      "iter 588: loss 0.0794, time: 547.58ms\n",
      "iter 589: loss 0.0703, time: 546.47ms\n",
      "iter 590: loss 0.0736, time: 541.90ms\n",
      "iter 591: loss 0.0610, time: 547.78ms\n",
      "iter 592: loss 0.0928, time: 548.76ms\n",
      "iter 593: loss 0.0796, time: 549.80ms\n",
      "iter 594: loss 0.0741, time: 545.66ms\n",
      "iter 595: loss 0.0846, time: 544.32ms\n",
      "iter 596: loss 0.0742, time: 547.24ms\n",
      "iter 597: loss 0.0647, time: 531.84ms\n",
      "iter 598: loss 0.0730, time: 551.49ms\n",
      "iter 599: loss 0.0927, time: 548.83ms\n",
      "Validating ...\n",
      "step 600: val loss 1.4260\n",
      "Saving checkpoint to out/training\n",
      "iter 600: loss 0.0776, time: 717.50ms\n",
      "iter 601: loss 0.0769, time: 524.07ms\n",
      "iter 602: loss 0.0640, time: 523.03ms\n",
      "iter 603: loss 0.0903, time: 524.14ms\n",
      "iter 604: loss 0.0896, time: 524.31ms\n",
      "iter 605: loss 0.0874, time: 525.92ms\n",
      "iter 606: loss 0.0759, time: 525.25ms\n",
      "iter 607: loss 0.0625, time: 525.50ms\n",
      "iter 608: loss 0.1041, time: 526.87ms\n",
      "iter 609: loss 0.0563, time: 527.86ms\n",
      "iter 610: loss 0.0948, time: 527.88ms\n",
      "iter 611: loss 0.1089, time: 527.61ms\n",
      "iter 612: loss 0.0706, time: 527.94ms\n",
      "iter 613: loss 0.0824, time: 530.97ms\n",
      "iter 614: loss 0.0765, time: 527.91ms\n",
      "iter 615: loss 0.0695, time: 530.86ms\n",
      "iter 616: loss 0.0877, time: 533.17ms\n",
      "iter 617: loss 0.0771, time: 531.62ms\n",
      "iter 618: loss 0.0812, time: 533.78ms\n",
      "iter 619: loss 0.0950, time: 534.14ms\n",
      "iter 620: loss 0.0710, time: 534.11ms\n",
      "iter 621: loss 0.0981, time: 533.25ms\n",
      "iter 622: loss 0.0851, time: 535.72ms\n",
      "iter 623: loss 0.0754, time: 534.91ms\n",
      "iter 624: loss 0.0742, time: 532.89ms\n",
      "iter 625: loss 0.0887, time: 537.77ms\n",
      "iter 626: loss 0.0879, time: 538.06ms\n",
      "iter 627: loss 0.1122, time: 535.47ms\n",
      "iter 628: loss 0.0721, time: 538.40ms\n",
      "iter 629: loss 0.0962, time: 539.01ms\n",
      "iter 630: loss 0.0797, time: 539.14ms\n",
      "iter 631: loss 0.0636, time: 537.70ms\n",
      "iter 632: loss 0.0662, time: 535.60ms\n",
      "iter 633: loss 0.0765, time: 542.95ms\n",
      "iter 634: loss 0.0984, time: 540.32ms\n",
      "iter 635: loss 0.0999, time: 531.02ms\n",
      "iter 636: loss 0.0571, time: 543.76ms\n",
      "iter 637: loss 0.0895, time: 542.29ms\n",
      "iter 638: loss 0.1006, time: 539.67ms\n",
      "iter 639: loss 0.0732, time: 542.54ms\n",
      "iter 640: loss 0.0820, time: 538.44ms\n",
      "iter 641: loss 0.0815, time: 541.95ms\n",
      "iter 642: loss 0.0794, time: 543.66ms\n",
      "iter 643: loss 0.0782, time: 539.74ms\n",
      "iter 644: loss 0.0788, time: 542.95ms\n",
      "iter 645: loss 0.0849, time: 543.81ms\n",
      "iter 646: loss 0.0576, time: 545.47ms\n",
      "iter 647: loss 0.0676, time: 544.61ms\n",
      "iter 648: loss 0.0802, time: 538.91ms\n",
      "iter 649: loss 0.0760, time: 546.08ms\n",
      "iter 650: loss 0.0693, time: 542.79ms\n",
      "iter 651: loss 0.0774, time: 544.58ms\n",
      "iter 652: loss 0.0681, time: 544.87ms\n",
      "iter 653: loss 0.0645, time: 541.94ms\n",
      "iter 654: loss 0.0587, time: 546.58ms\n",
      "iter 655: loss 0.0856, time: 532.10ms\n",
      "iter 656: loss 0.0712, time: 549.22ms\n",
      "iter 657: loss 0.0691, time: 547.01ms\n",
      "iter 658: loss 0.0727, time: 539.89ms\n",
      "iter 659: loss 0.0731, time: 547.41ms\n",
      "iter 660: loss 0.0732, time: 532.47ms\n",
      "iter 661: loss 0.0594, time: 552.10ms\n",
      "iter 662: loss 0.0686, time: 547.05ms\n",
      "iter 663: loss 0.0934, time: 547.88ms\n",
      "iter 664: loss 0.0613, time: 547.59ms\n",
      "iter 665: loss 0.0838, time: 543.16ms\n",
      "iter 666: loss 0.0849, time: 546.51ms\n",
      "iter 667: loss 0.0574, time: 548.42ms\n",
      "iter 668: loss 0.0886, time: 548.50ms\n",
      "iter 669: loss 0.0717, time: 547.42ms\n",
      "iter 670: loss 0.0655, time: 545.15ms\n",
      "iter 671: loss 0.0592, time: 548.81ms\n",
      "iter 672: loss 0.0772, time: 542.25ms\n",
      "iter 673: loss 0.0762, time: 546.31ms\n",
      "iter 674: loss 0.0778, time: 550.06ms\n",
      "iter 675: loss 0.0961, time: 547.88ms\n",
      "iter 676: loss 0.0443, time: 549.14ms\n",
      "iter 677: loss 0.0896, time: 544.92ms\n",
      "iter 678: loss 0.0651, time: 547.96ms\n",
      "iter 679: loss 0.0864, time: 532.87ms\n",
      "iter 680: loss 0.0746, time: 550.70ms\n",
      "iter 681: loss 0.0690, time: 548.12ms\n",
      "iter 682: loss 0.0575, time: 549.48ms\n",
      "iter 683: loss 0.0543, time: 547.31ms\n",
      "iter 684: loss 0.0763, time: 545.27ms\n",
      "iter 685: loss 0.0659, time: 548.26ms\n",
      "iter 686: loss 0.0608, time: 547.36ms\n",
      "iter 687: loss 0.0697, time: 547.78ms\n",
      "iter 688: loss 0.0632, time: 548.86ms\n",
      "iter 689: loss 0.0659, time: 548.51ms\n",
      "iter 690: loss 0.0613, time: 548.66ms\n",
      "iter 691: loss 0.1170, time: 544.21ms\n",
      "iter 692: loss 0.0557, time: 546.49ms\n",
      "iter 693: loss 0.0752, time: 551.10ms\n",
      "iter 694: loss 0.0582, time: 549.85ms\n",
      "iter 695: loss 0.0851, time: 549.43ms\n",
      "iter 696: loss 0.0606, time: 549.38ms\n",
      "iter 697: loss 0.0803, time: 547.74ms\n",
      "iter 698: loss 0.0909, time: 546.73ms\n",
      "iter 699: loss 0.0737, time: 549.10ms\n",
      "Validating ...\n",
      "step 700: val loss 1.4166\n",
      "Saving checkpoint to out/training\n",
      "iter 700: loss 0.0594, time: 719.88ms\n",
      "iter 701: loss 0.0723, time: 524.85ms\n",
      "iter 702: loss 0.0886, time: 523.68ms\n",
      "iter 703: loss 0.0573, time: 525.44ms\n",
      "iter 704: loss 0.0516, time: 525.01ms\n",
      "iter 705: loss 0.0822, time: 524.71ms\n",
      "iter 706: loss 0.0500, time: 526.48ms\n",
      "iter 707: loss 0.0514, time: 527.86ms\n",
      "iter 708: loss 0.0806, time: 527.92ms\n",
      "iter 709: loss 0.0802, time: 528.99ms\n",
      "iter 710: loss 0.1070, time: 530.94ms\n",
      "iter 711: loss 0.0552, time: 529.75ms\n",
      "iter 712: loss 0.0715, time: 529.86ms\n",
      "iter 713: loss 0.0924, time: 533.12ms\n",
      "iter 714: loss 0.0683, time: 533.15ms\n",
      "iter 715: loss 0.0683, time: 531.72ms\n",
      "iter 716: loss 0.0674, time: 534.07ms\n",
      "iter 717: loss 0.0664, time: 533.94ms\n",
      "iter 718: loss 0.0762, time: 529.94ms\n",
      "iter 719: loss 0.0569, time: 534.76ms\n",
      "iter 720: loss 0.0605, time: 537.42ms\n",
      "iter 721: loss 0.0632, time: 536.25ms\n",
      "iter 722: loss 0.0725, time: 536.11ms\n",
      "iter 723: loss 0.0555, time: 537.96ms\n",
      "iter 724: loss 0.0593, time: 537.48ms\n",
      "iter 725: loss 0.0873, time: 537.54ms\n",
      "iter 726: loss 0.0761, time: 532.77ms\n",
      "iter 727: loss 0.0635, time: 541.32ms\n",
      "iter 728: loss 0.0788, time: 538.61ms\n",
      "iter 729: loss 0.0885, time: 530.29ms\n",
      "iter 730: loss 0.0522, time: 542.45ms\n",
      "iter 731: loss 0.0900, time: 540.82ms\n",
      "iter 732: loss 0.0555, time: 537.07ms\n",
      "iter 733: loss 0.0601, time: 543.12ms\n",
      "iter 734: loss 0.0801, time: 542.22ms\n",
      "iter 735: loss 0.0573, time: 543.07ms\n",
      "iter 736: loss 0.0792, time: 543.29ms\n",
      "iter 737: loss 0.0657, time: 541.54ms\n",
      "iter 738: loss 0.0744, time: 543.19ms\n",
      "iter 739: loss 0.0610, time: 543.36ms\n",
      "iter 740: loss 0.0688, time: 543.12ms\n",
      "iter 741: loss 0.0709, time: 545.65ms\n",
      "iter 742: loss 0.0892, time: 542.36ms\n",
      "iter 743: loss 0.0953, time: 543.74ms\n",
      "iter 744: loss 0.0662, time: 546.13ms\n",
      "iter 745: loss 0.0717, time: 545.07ms\n",
      "iter 746: loss 0.0894, time: 543.22ms\n",
      "iter 747: loss 0.0630, time: 543.23ms\n",
      "iter 748: loss 0.0553, time: 545.22ms\n",
      "iter 749: loss 0.0809, time: 542.41ms\n",
      "iter 750: loss 0.0755, time: 546.83ms\n",
      "iter 751: loss 0.0756, time: 547.40ms\n",
      "iter 752: loss 0.0734, time: 542.75ms\n",
      "iter 753: loss 0.0558, time: 545.99ms\n",
      "iter 754: loss 0.0735, time: 536.73ms\n",
      "iter 755: loss 0.0630, time: 548.37ms\n",
      "iter 756: loss 0.0620, time: 547.63ms\n",
      "iter 757: loss 0.0919, time: 545.25ms\n",
      "iter 758: loss 0.0901, time: 548.55ms\n",
      "iter 759: loss 0.0547, time: 543.98ms\n",
      "iter 760: loss 0.0506, time: 550.19ms\n",
      "iter 761: loss 0.0667, time: 548.04ms\n",
      "iter 762: loss 0.0802, time: 546.42ms\n",
      "iter 763: loss 0.0820, time: 549.55ms\n",
      "iter 764: loss 0.0756, time: 546.69ms\n",
      "iter 765: loss 0.0636, time: 548.13ms\n",
      "iter 766: loss 0.0688, time: 539.34ms\n",
      "iter 767: loss 0.0753, time: 550.75ms\n",
      "iter 768: loss 0.0599, time: 550.26ms\n",
      "iter 769: loss 0.0588, time: 545.18ms\n",
      "iter 770: loss 0.0582, time: 548.89ms\n",
      "iter 771: loss 0.0561, time: 547.49ms\n",
      "iter 772: loss 0.0561, time: 548.89ms\n",
      "iter 773: loss 0.0684, time: 532.06ms\n",
      "iter 774: loss 0.0663, time: 553.89ms\n",
      "iter 775: loss 0.0957, time: 550.53ms\n",
      "iter 776: loss 0.0607, time: 547.41ms\n",
      "iter 777: loss 0.0813, time: 548.56ms\n",
      "iter 778: loss 0.0681, time: 540.96ms\n",
      "iter 779: loss 0.0395, time: 550.48ms\n",
      "iter 780: loss 0.0589, time: 545.88ms\n",
      "iter 781: loss 0.0817, time: 548.30ms\n",
      "iter 782: loss 0.0648, time: 550.11ms\n",
      "iter 783: loss 0.0745, time: 549.57ms\n",
      "iter 784: loss 0.0584, time: 550.02ms\n",
      "iter 785: loss 0.0673, time: 547.10ms\n",
      "iter 786: loss 0.0593, time: 547.78ms\n",
      "iter 787: loss 0.0742, time: 549.71ms\n",
      "iter 788: loss 0.0648, time: 549.84ms\n",
      "iter 789: loss 0.0834, time: 547.75ms\n",
      "iter 790: loss 0.0531, time: 551.26ms\n",
      "iter 791: loss 0.0696, time: 551.40ms\n",
      "iter 792: loss 0.0613, time: 547.19ms\n",
      "iter 793: loss 0.0896, time: 550.65ms\n",
      "iter 794: loss 0.0530, time: 533.98ms\n",
      "iter 795: loss 0.0596, time: 554.16ms\n",
      "iter 796: loss 0.0759, time: 551.11ms\n",
      "iter 797: loss 0.0594, time: 549.48ms\n",
      "iter 798: loss 0.0625, time: 550.79ms\n",
      "iter 799: loss 0.0641, time: 545.38ms\n",
      "Validating ...\n",
      "step 800: val loss 1.5376\n",
      "Saving checkpoint to out/training\n",
      "iter 800: loss 0.0782, time: 824.50ms\n",
      "iter 801: loss 0.0513, time: 524.19ms\n",
      "iter 802: loss 0.0403, time: 523.95ms\n",
      "iter 803: loss 0.0586, time: 526.71ms\n",
      "iter 804: loss 0.1345, time: 526.12ms\n",
      "iter 805: loss 0.0681, time: 524.17ms\n",
      "iter 806: loss 0.0453, time: 528.64ms\n",
      "iter 807: loss 0.0565, time: 528.52ms\n",
      "iter 808: loss 0.0819, time: 526.53ms\n",
      "iter 809: loss 0.1006, time: 529.41ms\n",
      "iter 810: loss 0.0751, time: 531.74ms\n",
      "iter 811: loss 0.0828, time: 529.55ms\n",
      "iter 812: loss 0.1046, time: 526.69ms\n",
      "iter 813: loss 0.0727, time: 533.17ms\n",
      "iter 814: loss 0.0938, time: 533.62ms\n",
      "iter 815: loss 0.0654, time: 532.76ms\n",
      "iter 816: loss 0.0589, time: 533.65ms\n",
      "iter 817: loss 0.0559, time: 534.11ms\n",
      "iter 818: loss 0.0552, time: 530.02ms\n",
      "iter 819: loss 0.0706, time: 534.08ms\n",
      "iter 820: loss 0.0701, time: 535.96ms\n",
      "iter 821: loss 0.0735, time: 534.77ms\n",
      "iter 822: loss 0.0709, time: 535.88ms\n",
      "iter 823: loss 0.0559, time: 537.59ms\n",
      "iter 824: loss 0.0684, time: 535.79ms\n",
      "iter 825: loss 0.0854, time: 538.85ms\n",
      "iter 826: loss 0.0618, time: 539.30ms\n",
      "iter 827: loss 0.0548, time: 538.04ms\n",
      "iter 828: loss 0.0970, time: 539.51ms\n",
      "iter 829: loss 0.0482, time: 534.02ms\n",
      "iter 830: loss 0.0684, time: 542.91ms\n",
      "iter 831: loss 0.0622, time: 542.45ms\n",
      "iter 832: loss 0.0765, time: 535.08ms\n",
      "iter 833: loss 0.0686, time: 544.93ms\n",
      "iter 834: loss 0.0780, time: 541.75ms\n",
      "iter 835: loss 0.0617, time: 542.28ms\n",
      "iter 836: loss 0.0777, time: 542.68ms\n",
      "iter 837: loss 0.0644, time: 532.03ms\n",
      "iter 838: loss 0.0737, time: 545.69ms\n",
      "iter 839: loss 0.0684, time: 545.40ms\n",
      "iter 840: loss 0.0552, time: 541.04ms\n",
      "iter 841: loss 0.0683, time: 544.82ms\n",
      "iter 842: loss 0.0773, time: 531.47ms\n",
      "iter 843: loss 0.0845, time: 545.81ms\n",
      "iter 844: loss 0.0549, time: 548.25ms\n",
      "iter 845: loss 0.0846, time: 546.51ms\n",
      "iter 846: loss 0.0677, time: 546.16ms\n",
      "iter 847: loss 0.0624, time: 534.89ms\n",
      "iter 848: loss 0.0923, time: 551.12ms\n",
      "iter 849: loss 0.0733, time: 548.20ms\n",
      "iter 850: loss 0.0746, time: 546.87ms\n",
      "iter 851: loss 0.0628, time: 547.44ms\n",
      "iter 852: loss 0.0644, time: 544.31ms\n",
      "iter 853: loss 0.0792, time: 547.15ms\n",
      "iter 854: loss 0.0558, time: 544.66ms\n",
      "iter 855: loss 0.0790, time: 548.86ms\n",
      "iter 856: loss 0.0649, time: 549.96ms\n",
      "iter 857: loss 0.0563, time: 546.87ms\n",
      "iter 858: loss 0.0544, time: 549.98ms\n",
      "iter 859: loss 0.0618, time: 543.10ms\n",
      "iter 860: loss 0.0750, time: 549.94ms\n",
      "iter 861: loss 0.0647, time: 550.59ms\n",
      "iter 862: loss 0.0690, time: 550.18ms\n",
      "iter 863: loss 0.0448, time: 551.18ms\n",
      "iter 864: loss 0.0632, time: 549.20ms\n",
      "iter 865: loss 0.0856, time: 549.75ms\n",
      "iter 866: loss 0.0565, time: 544.02ms\n",
      "iter 867: loss 0.0621, time: 549.36ms\n",
      "iter 868: loss 0.0490, time: 541.33ms\n",
      "iter 869: loss 0.0624, time: 551.13ms\n",
      "iter 870: loss 0.0575, time: 550.73ms\n",
      "iter 871: loss 0.0881, time: 548.49ms\n",
      "iter 872: loss 0.0590, time: 550.70ms\n",
      "iter 873: loss 0.0474, time: 548.21ms\n",
      "iter 874: loss 0.0727, time: 549.47ms\n",
      "iter 875: loss 0.0607, time: 547.86ms\n",
      "iter 876: loss 0.0841, time: 549.82ms\n",
      "iter 877: loss 0.0757, time: 551.06ms\n",
      "iter 878: loss 0.0615, time: 550.40ms\n",
      "iter 879: loss 0.0549, time: 550.44ms\n",
      "iter 880: loss 0.0598, time: 548.45ms\n",
      "iter 881: loss 0.0494, time: 550.82ms\n",
      "iter 882: loss 0.0509, time: 548.21ms\n",
      "iter 883: loss 0.0740, time: 551.03ms\n",
      "iter 884: loss 0.0677, time: 547.81ms\n",
      "iter 885: loss 0.0477, time: 547.86ms\n",
      "iter 886: loss 0.0673, time: 550.29ms\n",
      "iter 887: loss 0.0473, time: 552.45ms\n",
      "iter 888: loss 0.0520, time: 551.37ms\n",
      "iter 889: loss 0.0509, time: 549.07ms\n",
      "iter 890: loss 0.0705, time: 548.31ms\n",
      "iter 891: loss 0.0481, time: 533.69ms\n",
      "iter 892: loss 0.0738, time: 556.01ms\n",
      "iter 893: loss 0.0542, time: 551.59ms\n",
      "iter 894: loss 0.0599, time: 551.71ms\n",
      "iter 895: loss 0.0814, time: 552.35ms\n",
      "iter 896: loss 0.0746, time: 548.59ms\n",
      "iter 897: loss 0.0592, time: 550.94ms\n",
      "iter 898: loss 0.0850, time: 544.99ms\n",
      "iter 899: loss 0.0736, time: 551.38ms\n",
      "Validating ...\n",
      "step 900: val loss 1.5331\n",
      "Saving checkpoint to out/training\n",
      "iter 900: loss 0.0813, time: 726.09ms\n",
      "iter 901: loss 0.0640, time: 526.07ms\n",
      "iter 902: loss 0.0711, time: 527.18ms\n",
      "iter 903: loss 0.0663, time: 525.62ms\n",
      "iter 904: loss 0.0622, time: 526.43ms\n",
      "iter 905: loss 0.0608, time: 525.37ms\n",
      "iter 906: loss 0.0824, time: 528.67ms\n",
      "iter 907: loss 0.0622, time: 528.67ms\n",
      "iter 908: loss 0.0819, time: 530.01ms\n",
      "iter 909: loss 0.0672, time: 527.70ms\n",
      "iter 910: loss 0.0542, time: 530.25ms\n",
      "iter 911: loss 0.0563, time: 530.59ms\n",
      "iter 912: loss 0.0654, time: 532.38ms\n",
      "iter 913: loss 0.0517, time: 534.98ms\n",
      "iter 914: loss 0.0788, time: 534.81ms\n",
      "iter 915: loss 0.0516, time: 530.16ms\n",
      "iter 916: loss 0.0700, time: 534.82ms\n",
      "iter 917: loss 0.0746, time: 535.63ms\n",
      "iter 918: loss 0.0583, time: 533.14ms\n",
      "iter 919: loss 0.0661, time: 536.07ms\n",
      "iter 920: loss 0.0481, time: 538.90ms\n",
      "iter 921: loss 0.0591, time: 536.80ms\n",
      "iter 922: loss 0.0592, time: 536.97ms\n",
      "iter 923: loss 0.0550, time: 538.68ms\n",
      "iter 924: loss 0.0517, time: 538.26ms\n",
      "iter 925: loss 0.0624, time: 537.88ms\n",
      "iter 926: loss 0.0801, time: 532.10ms\n",
      "iter 927: loss 0.0639, time: 544.78ms\n",
      "iter 928: loss 0.0884, time: 542.48ms\n",
      "iter 929: loss 0.0495, time: 533.80ms\n",
      "iter 930: loss 0.0719, time: 544.38ms\n",
      "iter 931: loss 0.0554, time: 543.59ms\n",
      "iter 932: loss 0.0446, time: 542.14ms\n",
      "iter 933: loss 0.0708, time: 544.07ms\n",
      "iter 934: loss 0.0862, time: 531.09ms\n",
      "iter 935: loss 0.0676, time: 548.52ms\n",
      "iter 936: loss 0.0651, time: 544.70ms\n",
      "iter 937: loss 0.0457, time: 542.15ms\n",
      "iter 938: loss 0.0732, time: 545.57ms\n",
      "iter 939: loss 0.0541, time: 531.40ms\n",
      "iter 940: loss 0.0608, time: 546.77ms\n",
      "iter 941: loss 0.0638, time: 547.53ms\n",
      "iter 942: loss 0.0520, time: 544.67ms\n",
      "iter 943: loss 0.0629, time: 546.14ms\n",
      "iter 944: loss 0.0634, time: 544.28ms\n",
      "iter 945: loss 0.0620, time: 548.30ms\n",
      "iter 946: loss 0.0512, time: 546.23ms\n",
      "iter 947: loss 0.0612, time: 548.05ms\n",
      "iter 948: loss 0.0768, time: 548.22ms\n",
      "iter 949: loss 0.0611, time: 541.73ms\n",
      "iter 950: loss 0.0658, time: 547.85ms\n",
      "iter 951: loss 0.0693, time: 535.40ms\n",
      "iter 952: loss 0.0754, time: 553.08ms\n",
      "iter 953: loss 0.0693, time: 549.80ms\n",
      "iter 954: loss 0.0585, time: 547.14ms\n",
      "iter 955: loss 0.0875, time: 550.43ms\n",
      "iter 956: loss 0.0551, time: 544.10ms\n",
      "iter 957: loss 0.0803, time: 549.31ms\n",
      "iter 958: loss 0.0461, time: 548.90ms\n",
      "iter 959: loss 0.0627, time: 549.39ms\n",
      "iter 960: loss 0.0636, time: 550.00ms\n",
      "iter 961: loss 0.0566, time: 548.19ms\n",
      "iter 962: loss 0.0788, time: 550.50ms\n",
      "iter 963: loss 0.0536, time: 545.60ms\n",
      "iter 964: loss 0.0728, time: 549.09ms\n",
      "iter 965: loss 0.0796, time: 543.62ms\n",
      "iter 966: loss 0.0768, time: 552.25ms\n",
      "iter 967: loss 0.0555, time: 550.43ms\n",
      "iter 968: loss 0.0674, time: 552.10ms\n",
      "iter 969: loss 0.0888, time: 550.72ms\n",
      "iter 970: loss 0.0458, time: 546.70ms\n",
      "iter 971: loss 0.0504, time: 550.49ms\n",
      "iter 972: loss 0.0545, time: 546.73ms\n",
      "iter 973: loss 0.0662, time: 549.29ms\n",
      "iter 974: loss 0.0608, time: 546.43ms\n",
      "iter 975: loss 0.0518, time: 552.75ms\n",
      "iter 976: loss 0.0706, time: 550.46ms\n",
      "iter 977: loss 0.0704, time: 550.29ms\n",
      "iter 978: loss 0.1538, time: 550.06ms\n",
      "iter 979: loss 0.0711, time: 546.13ms\n",
      "iter 980: loss 0.0514, time: 551.95ms\n",
      "iter 981: loss 0.0625, time: 533.30ms\n",
      "iter 982: loss 0.0828, time: 557.93ms\n",
      "iter 983: loss 0.0634, time: 552.60ms\n",
      "iter 984: loss 0.0741, time: 551.23ms\n",
      "iter 985: loss 0.0643, time: 553.19ms\n",
      "iter 986: loss 0.0910, time: 549.41ms\n",
      "iter 987: loss 0.0654, time: 550.50ms\n",
      "iter 988: loss 0.0510, time: 533.27ms\n",
      "iter 989: loss 0.0948, time: 555.34ms\n",
      "iter 990: loss 0.0567, time: 553.24ms\n",
      "iter 991: loss 0.0548, time: 551.19ms\n",
      "iter 992: loss 0.0536, time: 552.19ms\n",
      "iter 993: loss 0.0501, time: 550.16ms\n",
      "iter 994: loss 0.0741, time: 551.18ms\n",
      "iter 995: loss 0.0581, time: 550.76ms\n",
      "iter 996: loss 0.0487, time: 552.10ms\n",
      "iter 997: loss 0.0495, time: 545.24ms\n",
      "iter 998: loss 0.0583, time: 551.98ms\n",
      "iter 999: loss 0.0692, time: 553.56ms\n",
      "Validating ...\n",
      "step 1000: val loss 1.6126\n",
      "Saving checkpoint to out/training\n",
      "iter 1000: loss 0.0874, time: 763.07ms\n",
      "iter 1001: loss 0.0751, time: 527.34ms\n",
      "iter 1002: loss 0.0595, time: 525.39ms\n",
      "iter 1003: loss 0.0544, time: 524.92ms\n",
      "iter 1004: loss 0.0490, time: 527.59ms\n",
      "iter 1005: loss 0.0663, time: 527.59ms\n",
      "iter 1006: loss 0.0610, time: 527.84ms\n",
      "iter 1007: loss 0.0672, time: 530.86ms\n",
      "iter 1008: loss 0.0667, time: 530.06ms\n",
      "iter 1009: loss 0.0671, time: 529.45ms\n",
      "iter 1010: loss 0.0616, time: 530.67ms\n",
      "iter 1011: loss 0.0683, time: 532.74ms\n",
      "iter 1012: loss 0.0661, time: 532.59ms\n",
      "iter 1013: loss 0.0519, time: 534.29ms\n",
      "iter 1014: loss 0.0650, time: 533.35ms\n",
      "iter 1015: loss 0.0682, time: 532.39ms\n",
      "iter 1016: loss 0.0545, time: 535.90ms\n",
      "iter 1017: loss 0.0575, time: 537.09ms\n",
      "iter 1018: loss 0.0605, time: 531.04ms\n",
      "iter 1019: loss 0.0542, time: 536.74ms\n",
      "iter 1020: loss 0.0593, time: 537.55ms\n",
      "iter 1021: loss 0.0788, time: 530.65ms\n",
      "iter 1022: loss 0.0633, time: 539.30ms\n",
      "iter 1023: loss 0.0594, time: 538.71ms\n",
      "iter 1024: loss 0.0756, time: 537.96ms\n",
      "iter 1025: loss 0.0659, time: 542.38ms\n",
      "iter 1026: loss 0.0525, time: 539.61ms\n",
      "iter 1027: loss 0.0725, time: 540.75ms\n",
      "iter 1028: loss 0.0690, time: 543.31ms\n",
      "iter 1029: loss 0.0690, time: 540.22ms\n",
      "iter 1030: loss 0.0627, time: 542.86ms\n",
      "iter 1031: loss 0.0556, time: 543.24ms\n",
      "iter 1032: loss 0.0460, time: 544.68ms\n",
      "iter 1033: loss 0.0513, time: 543.84ms\n",
      "iter 1034: loss 0.0515, time: 541.70ms\n",
      "iter 1035: loss 0.0692, time: 546.50ms\n",
      "iter 1036: loss 0.0579, time: 542.44ms\n",
      "iter 1037: loss 0.0454, time: 544.53ms\n",
      "iter 1038: loss 0.0770, time: 548.18ms\n",
      "iter 1039: loss 0.0489, time: 543.84ms\n",
      "iter 1040: loss 0.0575, time: 547.59ms\n",
      "iter 1041: loss 0.0618, time: 531.16ms\n",
      "iter 1042: loss 0.0618, time: 550.14ms\n",
      "iter 1043: loss 0.0777, time: 547.60ms\n",
      "iter 1044: loss 0.0562, time: 543.79ms\n",
      "iter 1045: loss 0.0660, time: 547.47ms\n",
      "iter 1046: loss 0.0743, time: 543.16ms\n",
      "iter 1047: loss 0.0591, time: 551.59ms\n",
      "iter 1048: loss 0.0706, time: 549.37ms\n",
      "iter 1049: loss 0.0573, time: 550.34ms\n",
      "iter 1050: loss 0.0531, time: 549.09ms\n",
      "iter 1051: loss 0.0530, time: 548.86ms\n",
      "iter 1052: loss 0.0713, time: 549.13ms\n",
      "iter 1053: loss 0.0549, time: 542.25ms\n",
      "iter 1054: loss 0.0418, time: 549.65ms\n",
      "iter 1055: loss 0.0583, time: 545.98ms\n",
      "iter 1056: loss 0.0767, time: 547.91ms\n",
      "iter 1057: loss 0.0559, time: 548.49ms\n",
      "iter 1058: loss 0.0751, time: 549.39ms\n",
      "iter 1059: loss 0.0633, time: 551.41ms\n",
      "iter 1060: loss 0.0527, time: 546.71ms\n",
      "iter 1061: loss 0.0559, time: 549.39ms\n",
      "iter 1062: loss 0.0533, time: 537.60ms\n",
      "iter 1063: loss 0.0595, time: 553.64ms\n",
      "iter 1064: loss 0.0747, time: 551.95ms\n",
      "iter 1065: loss 0.0614, time: 553.48ms\n",
      "iter 1066: loss 0.0627, time: 550.64ms\n",
      "iter 1067: loss 0.0515, time: 548.10ms\n",
      "iter 1068: loss 0.0644, time: 550.39ms\n",
      "iter 1069: loss 0.0571, time: 548.77ms\n",
      "iter 1070: loss 0.0958, time: 551.95ms\n",
      "iter 1071: loss 0.0964, time: 534.98ms\n",
      "iter 1072: loss 0.0629, time: 556.28ms\n",
      "iter 1073: loss 0.0506, time: 552.04ms\n",
      "iter 1074: loss 0.0662, time: 551.18ms\n",
      "iter 1075: loss 0.0483, time: 552.13ms\n",
      "iter 1076: loss 0.0501, time: 548.81ms\n",
      "iter 1077: loss 0.0822, time: 553.11ms\n",
      "iter 1078: loss 0.0736, time: 547.07ms\n",
      "iter 1079: loss 0.0466, time: 551.70ms\n",
      "iter 1080: loss 0.0837, time: 544.30ms\n",
      "iter 1081: loss 0.0639, time: 552.24ms\n",
      "iter 1082: loss 0.0608, time: 553.34ms\n",
      "iter 1083: loss 0.0595, time: 554.13ms\n",
      "iter 1084: loss 0.0587, time: 550.89ms\n",
      "iter 1085: loss 0.0705, time: 547.83ms\n",
      "iter 1086: loss 0.0561, time: 551.19ms\n",
      "iter 1087: loss 0.0819, time: 545.98ms\n",
      "iter 1088: loss 0.0634, time: 553.85ms\n",
      "iter 1089: loss 0.0470, time: 542.83ms\n",
      "iter 1090: loss 0.0628, time: 552.18ms\n",
      "iter 1091: loss 0.0830, time: 553.05ms\n",
      "iter 1092: loss 0.0714, time: 556.13ms\n",
      "iter 1093: loss 0.0680, time: 554.65ms\n",
      "iter 1094: loss 0.0610, time: 549.95ms\n",
      "iter 1095: loss 0.0519, time: 554.31ms\n",
      "iter 1096: loss 0.0625, time: 546.10ms\n",
      "iter 1097: loss 0.0765, time: 553.84ms\n",
      "iter 1098: loss 0.0469, time: 533.51ms\n",
      "iter 1099: loss 0.0399, time: 557.15ms\n",
      "Validating ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects\\LLaMA-Mol\\2_fine_tune.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/2_fine_tune.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Training\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/2_fine_tune.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train(model, optimizer, train_data, val_data)\n",
      "\u001b[1;32md:\\Projects\\LLaMA-Mol\\2_fine_tune.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/2_fine_tune.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/2_fine_tune.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m# TODO: add learning rate scheduling\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/2_fine_tune.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/2_fine_tune.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# evaluate the loss on train/val sets and write checkpoints\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/2_fine_tune.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mif\u001b[39;00m iter_num \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m iter_num \u001b[39m%\u001b[39m eval_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/2_fine_tune.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         val_loss \u001b[39m=\u001b[39m validate(model, val_data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/2_fine_tune.ipynb#X15sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstep \u001b[39m\u001b[39m{\u001b[39;00miter_num\u001b[39m}\u001b[39;00m\u001b[39m: val loss \u001b[39m\u001b[39m{\u001b[39;00mval_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/2_fine_tune.ipynb#X15sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSaving checkpoint to \u001b[39m\u001b[39m{\u001b[39;00mout_dir\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\m1000\\anaconda3\\envs\\my-rdkit-env\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;32md:\\Projects\\LLaMA-Mol\\2_fine_tune.ipynb Cell 15\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/2_fine_tune.ipynb#X15sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m input_ids, targets \u001b[39m=\u001b[39m get_batch(val_data, block_size\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mblock_size)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/2_fine_tune.ipynb#X15sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m input_ids, targets \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mto(device), targets\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/2_fine_tune.ipynb#X15sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m logits \u001b[39m=\u001b[39m model(input_ids)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/2_fine_tune.ipynb#X15sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mcross_entropy(logits\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, logits\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)), targets\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), ignore_index\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/2_fine_tune.ipynb#X15sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m losses[k] \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\m1000\\anaconda3\\envs\\my-rdkit-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\Projects\\LLaMA-Mol\\2_fine_tune.ipynb Cell 15\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/2_fine_tune.ipynb#X15sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mif\u001b[39;00m input_pos \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# proxy for use_cache=False\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/2_fine_tune.ipynb#X15sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mh:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/2_fine_tune.ipynb#X15sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m         x, _ \u001b[39m=\u001b[39m block(x, rope, mask, max_seq_length)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/2_fine_tune.ipynb#X15sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/2_fine_tune.ipynb#X15sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkv_caches:\n",
      "File \u001b[1;32mc:\\Users\\m1000\\anaconda3\\envs\\my-rdkit-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\Projects\\LLaMA-Mol\\2_fine_tune.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/2_fine_tune.ipynb#X15sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m h, new_kv_cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrms_1(x), rope, mask, max_seq_length, input_pos, kv_cache)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/2_fine_tune.ipynb#X15sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m h\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/2_fine_tune.ipynb#X15sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrms_2(x))\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/2_fine_tune.ipynb#X15sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x, new_kv_cache\n",
      "File \u001b[1;32mc:\\Users\\m1000\\anaconda3\\envs\\my-rdkit-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\Projects\\LLaMA-Mol\\2_fine_tune.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/2_fine_tune.ipynb#X15sZmlsZQ%3D%3D?line=200'>201</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/2_fine_tune.ipynb#X15sZmlsZQ%3D%3D?line=201'>202</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msilu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_fc1(x)) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_fc2(x)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/2_fine_tune.ipynb#X15sZmlsZQ%3D%3D?line=202'>203</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(x)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Projects/LLaMA-Mol/2_fine_tune.ipynb#X15sZmlsZQ%3D%3D?line=203'>204</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\m1000\\anaconda3\\envs\\my-rdkit-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1601\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1598\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_backward_pre_hooks\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[0;32m   1599\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39m=\u001b[39m OrderedDict()\n\u001b[1;32m-> 1601\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tensor, \u001b[39m'\u001b[39m\u001b[39mModule\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m   1602\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[0;32m   1603\u001b[0m         _parameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "train(model, optimizer, train_data, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import torch\n",
    "import lightning as L\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "import functools\n",
    "import pickle\n",
    "\n",
    "def llama_model_lookup(checkpoint: dict) -> str:\n",
    "    \"\"\"Returns the LLaMA model name from the checkpoint.\n",
    "    \n",
    "    Checks the width of the lm_head.weight matrix, as these uniquely identify the model.\n",
    "    \"\"\"\n",
    "    embedding_size = checkpoint['transformer.wte.weight'].shape[1]\n",
    "    return llama_model_sizes[embedding_size]\n",
    "class NotYetLoadedTensor:\n",
    "    def __init__(self, metatensor, archiveinfo, storageinfo, rebuild_args):\n",
    "        self.metatensor = metatensor\n",
    "        self.archiveinfo = archiveinfo\n",
    "        self.storageinfo = storageinfo\n",
    "        self.rebuild_args = rebuild_args\n",
    "\n",
    "    @classmethod\n",
    "    def rebuild_from_type_v2(cls, func, new_type, args, state, *, archiveinfo=None):\n",
    "        ret = func(*args)\n",
    "        if isinstance(ret, NotYetLoadedTensor):\n",
    "            old_lt = ret._load_tensor\n",
    "\n",
    "            def _load_tensor():\n",
    "                t = old_lt()\n",
    "                return torch._tensor._rebuild_from_type_v2(\n",
    "                    lambda: t, new_type, (), state\n",
    "                )\n",
    "\n",
    "            ret._load_tensor = _load_tensor\n",
    "            return ret\n",
    "        return torch._tensor._rebuild_from_type_v2(func, new_type, args, state)\n",
    "\n",
    "    @classmethod\n",
    "    def rebuild_parameter(\n",
    "        cls, data, requires_grad, backward_hooks, *, archiveinfo=None\n",
    "    ):\n",
    "        if isinstance(data, NotYetLoadedTensor):\n",
    "            old_lt = data._load_tensor\n",
    "\n",
    "            def _load_tensor():\n",
    "                t = old_lt()\n",
    "                return torch._utils._rebuild_parameter(t, requires_grad, backward_hooks)\n",
    "\n",
    "            data._load_tensor = _load_tensor\n",
    "            return data\n",
    "        return torch._utils._rebuild_parameter(data, requires_grad, backward_hooks)\n",
    "\n",
    "    @classmethod\n",
    "    def rebuild_tensor_v2(\n",
    "        cls,\n",
    "        storage,\n",
    "        storage_offset,\n",
    "        size,\n",
    "        stride,\n",
    "        requires_grad,\n",
    "        backward_hooks,\n",
    "        metadata=None,\n",
    "        *,\n",
    "        archiveinfo=None,\n",
    "    ):\n",
    "        rebuild_args = (\n",
    "            storage_offset,\n",
    "            size,\n",
    "            stride,\n",
    "            requires_grad,\n",
    "            backward_hooks,\n",
    "            metadata,\n",
    "        )\n",
    "        metatensor = torch._utils._rebuild_tensor_v2(\n",
    "            storage,\n",
    "            storage_offset,\n",
    "            size,\n",
    "            stride,\n",
    "            requires_grad,\n",
    "            backward_hooks,\n",
    "            metadata,\n",
    "        )\n",
    "        storageinfo = storage.archiveinfo\n",
    "        return NotYetLoadedTensor(metatensor, archiveinfo, storageinfo, rebuild_args)\n",
    "\n",
    "    def _load_tensor(self):\n",
    "        name, storage_cls, fn, device, size = self.storageinfo\n",
    "        dtype = self.metatensor.dtype\n",
    "\n",
    "        uts = (\n",
    "            self.archiveinfo.zipfile_context.zf.get_storage_from_record(\n",
    "                f\"data/{fn}\",\n",
    "                size * torch._utils._element_size(dtype),\n",
    "                torch.UntypedStorage,\n",
    "            )\n",
    "            ._typed_storage()\n",
    "            ._untyped_storage\n",
    "        )\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            storage = torch.storage.TypedStorage(\n",
    "                wrap_storage=uts, dtype=self.metatensor.dtype, _internal=True\n",
    "            )\n",
    "        tensor = torch._utils._rebuild_tensor_v2(storage, *self.rebuild_args)\n",
    "        return tensor\n",
    "\n",
    "    @classmethod\n",
    "    def __torch_function__(cls, func, types, args=(), kwargs=None):\n",
    "        if kwargs is None:\n",
    "            kwargs = {}\n",
    "        loaded_args = [\n",
    "            (a._load_tensor() if isinstance(a, NotYetLoadedTensor) else a) for a in args\n",
    "        ]\n",
    "        res = func(*loaded_args, **kwargs)\n",
    "        # gc.collect would be costly here, maybe do it optionally\n",
    "        return res\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        # properties\n",
    "        ## TODO: device, is_...??\n",
    "        ## TODO: mH, mT, H, T, data, imag, real\n",
    "        ## name ???\n",
    "        if name in {\n",
    "            \"dtype\",\n",
    "            \"grad\",\n",
    "            \"grad_fn\",\n",
    "            \"layout\",\n",
    "            \"names\",\n",
    "            \"ndim\",\n",
    "            \"output_nr\",\n",
    "            \"requires_grad\",\n",
    "            \"retains_grad\",\n",
    "            \"shape\",\n",
    "            \"volatile\",\n",
    "        }:\n",
    "            return getattr(self.metatensor, name)\n",
    "        if name in {\"size\"}:\n",
    "            return getattr(self.metatensor, name)\n",
    "        # materializing with contiguous is needed for quantization\n",
    "        if name in {\"contiguous\"}:\n",
    "            return getattr(self._load_tensor(), name)\n",
    "\n",
    "        raise AttributeError(f\"{type(self)} does not have {name}\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"NotYetLoadedTensor({repr(self.metatensor)})\"\n",
    "    \n",
    "class LazyLoadingUnpickler(pickle.Unpickler):\n",
    "    def __init__(self, file, zipfile_context):\n",
    "        super().__init__(file)\n",
    "        self.zipfile_context = zipfile_context\n",
    "\n",
    "    def find_class(self, module, name):\n",
    "        res = super().find_class(module, name)\n",
    "        if module == \"torch._utils\" and name == \"_rebuild_tensor_v2\":\n",
    "            return functools.partial(\n",
    "                NotYetLoadedTensor.rebuild_tensor_v2, archiveinfo=self\n",
    "            )\n",
    "        elif module == \"torch._tensor\" and name == \"_rebuild_from_type_v2\":\n",
    "            return functools.partial(\n",
    "                NotYetLoadedTensor.rebuild_from_type_v2, archiveinfo=self\n",
    "            )\n",
    "        elif module == \"torch._utils\" and name == \"_rebuild_parameter\":\n",
    "            return functools.partial(\n",
    "                NotYetLoadedTensor.rebuild_parameter, archiveinfo=self\n",
    "            )\n",
    "        return res\n",
    "\n",
    "    def persistent_load(self, pid):\n",
    "        name, cls, fn, device, size = pid\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            s = torch.storage.TypedStorage(dtype=cls().dtype, device=\"meta\")\n",
    "        s.archiveinfo = pid\n",
    "        return s\n",
    "    \n",
    "class lazy_load:\n",
    "    def __init__(self, fn):\n",
    "        self.zf = torch._C.PyTorchFileReader(str(fn))\n",
    "        with BytesIO(self.zf.get_record(\"data.pkl\")) as pkl:\n",
    "            mup = LazyLoadingUnpickler(pkl, self)\n",
    "            self.sd = mup.load()\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self.sd\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        del self.zf  # I don't think there is a way to force closing...\n",
    "        self.zf = None\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model: LLaMA,\n",
    "    idx: torch.Tensor,\n",
    "    max_new_tokens: int,\n",
    "    *,\n",
    "    max_seq_length: Optional[int] = None,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: Optional[int] = None,\n",
    "    eos_id: Optional[int] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Takes a conditioning sequence (prompt) as input and continues to generate as many tokens as requested.\n",
    "\n",
    "    The implementation of this function is modified from A. Karpathy's nanoGPT.\n",
    "\n",
    "    Args:\n",
    "        model: The model to use.\n",
    "        idx: Tensor of shape (T) with indices of the prompt sequence.\n",
    "        max_new_tokens: The number of new tokens to generate.\n",
    "        max_seq_length: The maximum sequence length allowed.\n",
    "        temperature: Scales the predicted logits by 1 / temperature\n",
    "        top_k: If specified, only sample among the tokens with the k highest probabilities\n",
    "        eos_id: If specified, stop generating any more token once the <eos> token is triggered\n",
    "    \"\"\"\n",
    "    # create an empty tensor of the expected final shape and fill in the current tokens\n",
    "    T = idx.size(0)\n",
    "    T_new = T + max_new_tokens\n",
    "    if max_seq_length is None:\n",
    "        max_seq_length = min(T_new, model.config.block_size)\n",
    "\n",
    "    device, dtype = idx.device, idx.dtype\n",
    "    # create an empty tensor of the expected final shape and fill in the current tokens\n",
    "    empty = torch.empty(T_new, dtype=dtype, device=device)\n",
    "    empty[:T] = idx\n",
    "    idx = empty\n",
    "    input_pos = torch.arange(0, T, device=device)\n",
    "\n",
    "    # if idx.device.type == \"xla\":\n",
    "    #     import torch_xla.core.xla_model as xm\n",
    "\n",
    "    #     xm.mark_step()\n",
    "\n",
    "    # generate max_new_tokens tokens\n",
    "    for _ in range(max_new_tokens):\n",
    "        x = idx.index_select(0, input_pos).view(1, -1)\n",
    "\n",
    "        # forward\n",
    "        logits = model(x, max_seq_length, input_pos)\n",
    "        logits = logits[0, -1] / temperature\n",
    "\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits = torch.where(logits < v[[-1]], -float(\"Inf\"), logits)\n",
    "\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1).to(dtype=dtype)\n",
    "\n",
    "        # advance\n",
    "        input_pos = input_pos[-1:] + 1\n",
    "\n",
    "        # if idx.device.type == \"xla\":\n",
    "        #     xm.mark_step()\n",
    "\n",
    "        # concatenate the new generation\n",
    "        idx = idx.index_copy(0, input_pos, idx_next)\n",
    "\n",
    "        # if <eos> token is triggered, return the output (stop generation)\n",
    "        if idx_next == eos_id:\n",
    "            return idx[:input_pos]  # include the EOS token\n",
    "\n",
    "    return idx\n",
    "\n",
    "# The main function can be called directly in the notebook\n",
    "# Load pre-trained model and tokenizer\n",
    "checkpoint_path = \"out/training/iter-001000-ckpt.pth\"\n",
    "tokenizer_path = \"data/smiles/tokenizer.model\"\n",
    "\n",
    "assert Path(checkpoint_path).is_file(), checkpoint_path\n",
    "assert Path(tokenizer_path).is_file(), tokenizer_path\n",
    "\n",
    "print(\"Loading model ...\", file=sys.stderr)\n",
    "t0 = time.time()\n",
    "with lazy_load(checkpoint_path) as checkpoint:    \n",
    "    model.load_state_dict(checkpoint)\n",
    "print(f\"Time to load model: {time.time() - t0:.02f} seconds.\", file=sys.stderr)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"CCC\"\n",
    "num_samples = 1\n",
    "max_new_tokens = 100\n",
    "top_k = 200\n",
    "temperature = 0.1\n",
    "\n",
    "model.eval()\n",
    "tokenizer = Tokenizer(tokenizer_path)\n",
    "\n",
    "encoded = tokenizer.encode(prompt, bos=True, eos=False, device=device)\n",
    "prompt_length = encoded.size(0)\n",
    "\n",
    "# L.seed_everything(1234)\n",
    "for i in range(num_samples):\n",
    "    t0 = time.perf_counter()\n",
    "    y = generate(model, encoded, max_new_tokens, temperature=temperature, top_k=top_k, eos_id=tokenizer.eos_id)\n",
    "    t = time.perf_counter() - t0\n",
    "\n",
    "    model.reset_cache()\n",
    "    smiles_string = tokenizer.decode(y)\n",
    "    print(smiles_string)\n",
    "    tokens_generated = y.size(0) - prompt_length\n",
    "    print(f\"Time for inference {i + 1}: {t:.02f} sec total, {tokens_generated / t:.02f} tokens/sec\", file=sys.stderr)\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Memory used: {torch.cuda.max_memory_reserved() / 1e9:.02f} GB\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 1024\n",
    "n_heads = 32\n",
    "multiple_of = 256\n",
    "n_blocks = 32\n",
    "\n",
    "head_dim = dim // n_heads\n",
    "n_attention_weights = 4 * dim * (n_heads * head_dim)\n",
    "\n",
    "hidden_dim = 4 * dim \n",
    "hidden_dim = int(2 * hidden_dim / 3)\n",
    "hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "n_feed_forward_weights = 3 * dim * hidden_dim\n",
    "\n",
    "total_weights = n_blocks * (n_attention_weights + n_feed_forward_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411041792"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204800"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * 100 * dim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
